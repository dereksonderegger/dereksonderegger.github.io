<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Introduction to Statistical Methodology</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Introduction to Statistical Methodology">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Introduction to Statistical Methodology" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="dereksonderegger/STA_570_Book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Introduction to Statistical Methodology" />
  
  
  

<meta name="author" content="Derek L. Sonderegger">


<meta name="date" content="2017-01-23">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="2-probability.html">
<link rel="next" href="4-sampling-distribution-of-barx.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Methods I</a></li>
<li><a href="https://dereksonderegger.github.io/570/Statistical_Methods_I.pdf" target="blank">PDF version</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Summary Statistics and Graphing</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#graphical-summaries-of-data"><i class="fa fa-check"></i><b>1.1</b> Graphical summaries of data</a><ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#univariate---categorical"><i class="fa fa-check"></i><b>1.1.1</b> Univariate - Categorical</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#univariate---continuous"><i class="fa fa-check"></i><b>1.1.2</b> Univariate - Continuous</a></li>
<li class="chapter" data-level="1.1.3" data-path="index.html"><a href="index.html#bivariate---categorical-vs-continuous"><i class="fa fa-check"></i><b>1.1.3</b> Bivariate - Categorical vs Continuous</a></li>
<li class="chapter" data-level="1.1.4" data-path="index.html"><a href="index.html#bivariate---continuous-vs-continuous"><i class="fa fa-check"></i><b>1.1.4</b> Bivariate - Continuous vs Continuous</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#measures-of-centrality"><i class="fa fa-check"></i><b>1.2</b> Measures of Centrality</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#mean"><i class="fa fa-check"></i><b>1.2.1</b> Mean</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#median"><i class="fa fa-check"></i><b>1.2.2</b> Median</a></li>
<li class="chapter" data-level="1.2.3" data-path="index.html"><a href="index.html#mode"><i class="fa fa-check"></i><b>1.2.3</b> Mode</a></li>
<li class="chapter" data-level="1.2.4" data-path="index.html"><a href="index.html#examples"><i class="fa fa-check"></i><b>1.2.4</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#measures-of-variation"><i class="fa fa-check"></i><b>1.3</b> Measures of Variation</a><ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#range"><i class="fa fa-check"></i><b>1.3.1</b> Range</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#inter-quartile-range"><i class="fa fa-check"></i><b>1.3.2</b> Inter-Quartile Range</a></li>
<li class="chapter" data-level="1.3.3" data-path="index.html"><a href="index.html#variance"><i class="fa fa-check"></i><b>1.3.3</b> Variance</a></li>
<li class="chapter" data-level="1.3.4" data-path="index.html"><a href="index.html#standard-deviation"><i class="fa fa-check"></i><b>1.3.4</b> Standard Deviation</a></li>
<li class="chapter" data-level="1.3.5" data-path="index.html"><a href="index.html#coefficient-of-variation"><i class="fa fa-check"></i><b>1.3.5</b> Coefficient of Variation</a></li>
<li class="chapter" data-level="1.3.6" data-path="index.html"><a href="index.html#empirical-rule-of-thumb"><i class="fa fa-check"></i><b>1.3.6</b> Empirical Rule of Thumb</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#exercises"><i class="fa fa-check"></i><b>1.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-probability.html"><a href="2-probability.html"><i class="fa fa-check"></i><b>2</b> Probability</a><ul>
<li class="chapter" data-level="2.1" data-path="2-probability.html"><a href="2-probability.html#introduction-to-set-theory"><i class="fa fa-check"></i><b>2.1</b> Introduction to Set Theory</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-probability.html"><a href="2-probability.html#composition-of-events"><i class="fa fa-check"></i><b>2.1.1</b> Composition of events</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-probability.html"><a href="2-probability.html#probability-rules"><i class="fa fa-check"></i><b>2.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-probability.html"><a href="2-probability.html#simple-rules"><i class="fa fa-check"></i><b>2.2.1</b> Simple Rules</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-probability.html"><a href="2-probability.html#conditional-probability"><i class="fa fa-check"></i><b>2.2.2</b> Conditional Probability</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-probability.html"><a href="2-probability.html#summary-of-probability-rules"><i class="fa fa-check"></i><b>2.2.3</b> Summary of Probability Rules</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-probability.html"><a href="2-probability.html#discrete-random-variables"><i class="fa fa-check"></i><b>2.3</b> Discrete Random Variables</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-probability.html"><a href="2-probability.html#introduction-to-discrete-random-variables"><i class="fa fa-check"></i><b>2.3.1</b> Introduction to Discrete Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-probability.html"><a href="2-probability.html#common-discrete-distributions"><i class="fa fa-check"></i><b>2.4</b> Common Discrete Distributions</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-probability.html"><a href="2-probability.html#binomial-distribution"><i class="fa fa-check"></i><b>2.4.1</b> Binomial Distribution</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-probability.html"><a href="2-probability.html#poisson-distribution"><i class="fa fa-check"></i><b>2.4.2</b> Poisson Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-probability.html"><a href="2-probability.html#continuous-random-variables"><i class="fa fa-check"></i><b>2.5</b> Continuous Random Variables</a><ul>
<li class="chapter" data-level="2.5.1" data-path="2-probability.html"><a href="2-probability.html#uniform01-distribution"><i class="fa fa-check"></i><b>2.5.1</b> Uniform(0,1) Distribution</a></li>
<li class="chapter" data-level="2.5.2" data-path="2-probability.html"><a href="2-probability.html#exponential-distribution"><i class="fa fa-check"></i><b>2.5.2</b> Exponential Distribution</a></li>
<li class="chapter" data-level="2.5.3" data-path="2-probability.html"><a href="2-probability.html#normal-distribution"><i class="fa fa-check"></i><b>2.5.3</b> Normal Distribution</a></li>
<li class="chapter" data-level="2.5.4" data-path="2-probability.html"><a href="2-probability.html#standardizing"><i class="fa fa-check"></i><b>2.5.4</b> Standardizing</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="2-probability.html"><a href="2-probability.html#exercises-1"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-confidence-intervals-via-bootstrapping.html"><a href="3-confidence-intervals-via-bootstrapping.html"><i class="fa fa-check"></i><b>3</b> Confidence Intervals via Bootstrapping</a><ul>
<li class="chapter" data-level="3.1" data-path="3-confidence-intervals-via-bootstrapping.html"><a href="3-confidence-intervals-via-bootstrapping.html#theory-of-bootstrapping"><i class="fa fa-check"></i><b>3.1</b> Theory of Bootstrapping</a></li>
<li class="chapter" data-level="3.2" data-path="3-confidence-intervals-via-bootstrapping.html"><a href="3-confidence-intervals-via-bootstrapping.html#quantile-based-confidence-intervals"><i class="fa fa-check"></i><b>3.2</b> Quantile-based Confidence Intervals</a></li>
<li class="chapter" data-level="3.3" data-path="3-confidence-intervals-via-bootstrapping.html"><a href="3-confidence-intervals-via-bootstrapping.html#exercises-2"><i class="fa fa-check"></i><b>3.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html"><i class="fa fa-check"></i><b>4</b> Sampling Distribution of <span class="math inline">\(\bar{X}\)</span></a><ul>
<li class="chapter" data-level="4.1" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#enlightening-example"><i class="fa fa-check"></i><b>4.1</b> Enlightening Example</a></li>
<li class="chapter" data-level="4.2" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#mathematical-details"><i class="fa fa-check"></i><b>4.2</b> Mathematical details</a><ul>
<li class="chapter" data-level="4.2.1" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#probability-rules-for-expectations-and-variances"><i class="fa fa-check"></i><b>4.2.1</b> Probability Rules for Expectations and Variances</a></li>
<li class="chapter" data-level="4.2.2" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#mean-and-variance-of-the-sample-mean"><i class="fa fa-check"></i><b>4.2.2</b> Mean and Variance of the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#distribution-of-barx"><i class="fa fa-check"></i><b>4.3</b> Distribution of <span class="math inline">\(\bar{X}\)</span></a></li>
<li class="chapter" data-level="4.4" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#central-limit-theorem"><i class="fa fa-check"></i><b>4.4</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="4.5" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#exercises-3"><i class="fa fa-check"></i><b>4.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-confidence-intervals-for-mu.html"><a href="5-confidence-intervals-for-mu.html"><i class="fa fa-check"></i><b>5</b> Confidence Intervals for <span class="math inline">\(\mu\)</span></a><ul>
<li class="chapter" data-level="5.1" data-path="5-confidence-intervals-for-mu.html"><a href="5-confidence-intervals-for-mu.html#asymptotic-result-sigma-known"><i class="fa fa-check"></i><b>5.1</b> Asymptotic result (<span class="math inline">\(\sigma\)</span> known)</a></li>
<li class="chapter" data-level="5.2" data-path="5-confidence-intervals-for-mu.html"><a href="5-confidence-intervals-for-mu.html#asymptotoic-result-sigma-unknown"><i class="fa fa-check"></i><b>5.2</b> Asymptotoic result (<span class="math inline">\(\sigma\)</span> unknown)</a></li>
<li class="chapter" data-level="5.3" data-path="5-confidence-intervals-for-mu.html"><a href="5-confidence-intervals-for-mu.html#sample-size-selection"><i class="fa fa-check"></i><b>5.3</b> Sample Size Selection</a></li>
<li class="chapter" data-level="5.4" data-path="5-confidence-intervals-for-mu.html"><a href="5-confidence-intervals-for-mu.html#exercises-4"><i class="fa fa-check"></i><b>5.4</b> Exercises</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Statistical Methodology</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="confidence-intervals-via-bootstrapping" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Confidence Intervals via Bootstrapping</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Every chapter, we will load all the librarys we will use at the beginning</span>
<span class="co"># of the chapter.</span>
<span class="kw">library</span>(mosaic)     <span class="co"># for the resample, shuffle, and do functions</span>
<span class="kw">library</span>(ggplot2)    <span class="co"># graphing functions</span>
<span class="kw">library</span>(dplyr)      <span class="co"># data summary tools</span></code></pre></div>
<div id="theory-of-bootstrapping" class="section level2">
<h2><span class="header-section-number">3.1</span> Theory of Bootstrapping</h2>
<p>Suppose that we had a population of interest and we wish to estimate the mean of that population (the population mean we’ll denote as <span class="math inline">\(\mu\)</span>). We can’t observe every member of the population (which would be prohibitively expensive) so instead we take a random sample and from that sample calculate a sample mean (which we’ll denote <span class="math inline">\(\bar{x}\)</span>). We believe that <span class="math inline">\(\bar{x}\)</span> will be a good estimator of <span class="math inline">\(\mu\)</span>, but it will vary from sample to sample and won’t be exactly equal to <span class="math inline">\(\mu\)</span>.</p>
<p>Next suppose we wish to ask if a particular value for <span class="math inline">\(\mu\)</span>, say <span class="math inline">\(\mu_{0}\)</span>, is consistent with our observed data? We know that <span class="math inline">\(\bar{x}\)</span> will vary from sample to sample, but we have no idea how much it will vary between samples. However, if we could understand how much <span class="math inline">\(\bar{x}\)</span> varied sample to sample, we could answer the question. For example, suppose that <span class="math inline">\(\bar{x}=5\)</span> and we know that <span class="math inline">\(\bar{x}\)</span> varied about <span class="math inline">\(\pm2\)</span> from sample to sample. Then I’d say that possible values of <span class="math inline">\(\mu_{0}\)</span> in the interval <span class="math inline">\(3\)</span> to <span class="math inline">\(7\)</span> <span class="math inline">\(\left(5\pm2\right)\)</span> are reasonable values for <span class="math inline">\(\mu\)</span> and anything outside that interval is not reasonable.</p>
<p>Therefore, if we could take many, many repeated samples from the population and calculate our test statistic <span class="math inline">\(\bar{x}\)</span> for each sample, we could rule out possible values of <span class="math inline">\(\mu\)</span>. Unfortunately we don’t have the time or money to repeatedly sample from the actual population, but we could sample from our best approximation to what the population is like.</p>
<p>Suppose we were to sample from a population of shapes, and we observed <span class="math inline">\(4/9\)</span> of the sample were squares, <span class="math inline">\(3/9\)</span> were circles, and a triangle and a diamond. Then our best guess of what the population that we sampled from was a population with <span class="math inline">\(4/9\)</span> squares, <span class="math inline">\(3/9\)</span> circles, and <span class="math inline">\(1/9\)</span> of triangles and diamonds.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">multiplot</span>(sample.plot, population.plot, <span class="dt">cols=</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## Loading required package: grid</code></pre>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-58-1.png" width="672" /></p>
<p>Using this approximated population (which is just many many copies of our sample data), we can repeated sample <span class="math inline">\(\bar{x}^{*}\)</span> values to create an estimate of the sampling distribution of <span class="math inline">\(\bar{x}\)</span>.</p>
<p>Because our approximate population is just an infinite number of copies of our sample data, then sampling from the approximate population is equivalent to sampling with replacement from our sample data. If I take <span class="math inline">\(n\)</span> samples from <span class="math inline">\(n\)</span> distinct objects with replacement, then the process can be thought of as mixing the <span class="math inline">\(n\)</span> objects in a bowl and taking an object at random, noting which it is, replace it into the bowl, and then draw the next sample. Practically, this means some objects will be selected more than once and some will not be chosen at all. To sample our observed data with replacement, we’ll use the <code>resample()</code> function in the <code>mosaic</code> package. We see that some rows will be selected multiple times, and some will not be selected at all.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Testing.Data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="dt">name=</span><span class="kw">c</span>(<span class="st">&#39;Alison&#39;</span>,<span class="st">&#39;Brandon&#39;</span>,<span class="st">&#39;Chelsea&#39;</span>,<span class="st">&#39;Derek&#39;</span>,<span class="st">&#39;Elise&#39;</span>))
Testing.Data</code></pre></div>
<pre><code>##      name
## 1  Alison
## 2 Brandon
## 3 Chelsea
## 4   Derek
## 5   Elise</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Sample rows from the Testing Data (with replacement)</span>
<span class="kw">resample</span>(Testing.Data)</code></pre></div>
<pre><code>##        name orig.id
## 1    Alison       1
## 4     Derek       4
## 3   Chelsea       3
## 1.1  Alison       1
## 5     Elise       5</code></pre>
<p>Notice Alison has selected twice, while Brandon has not been selected at all.</p>
<p>The sampling from the estimated population via sampling from the observed data is called bootstrapping because we are making no distributional assumptions about where the data came from, and the idiom “Pulling yourself up by your bootstraps” seemed appropriate.</p>
<p><strong>Example</strong>: Mercury Levels in Fish from Florida Lakes</p>
<p>A data set provided by the Lock<span class="math inline">\(^{5}\)</span> introductory statistics textbook looks at the mercury levels in fish harvested from lakes in Florida. There are approximately 7,700 lakes in Florida that are larger than 10 acres. As part of a study to assess the average mercury contamination in these lakes, a random sample of <span class="math inline">\(n=53\)</span> lakes, an unspecified number of fish were harvested and the average mercury level (in ppm) was calculated for fish in each lake. The goal of the study was to assess if the average mercury concentration was greater than the 1969 EPA “legally actionable level” of 0.5 ppm.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># read the Lakes data set</span>
Lakes &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&#39;http://www.lock5stat.com/datasets/FloridaLakes.csv&#39;</span>)
<span class="co"># make a nice picture... dot plots are very similar to histograms</span>
<span class="co"># but in this case, my y-axis doen&#39;t make any sense.</span>
<span class="kw">ggplot</span>(Lakes, <span class="kw">aes</span>(<span class="dt">x=</span>AvgMercury)) +
<span class="st">  </span><span class="kw">geom_dotplot</span>()</code></pre></div>
<pre><code>## `stat_bindot()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-61-1.png" width="672" /></p>
<p>We can calculate mean average mercury level for the <span class="math inline">\(n=53\)</span> lakes</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Lakes %&gt;%<span class="st"> </span><span class="kw">summarise</span>(<span class="dt">xbar =</span> <span class="kw">mean</span>( AvgMercury ))</code></pre></div>
<pre><code>##        xbar
## 1 0.5271698</code></pre>
<p>The sample mean is greater than <span class="math inline">\(0.5\)</span> but not by too much. Is a true population mean concentration <span class="math inline">\(\mu_{Hg}\)</span> that is <span class="math inline">\(0.5\)</span> or less incompatible with our observed data? Is our data sufficient evidence to conclude that the average mercury content is greater than <span class="math inline">\(0.5\)</span>? Perhaps the true average mercury content is less than (or equal to) <span class="math inline">\(0.5\)</span> and we just happened to get a random sample that with a mean greater than <span class="math inline">\(0.5\)</span>?</p>
<p>The first step in answering these questions is to create an estimate of the sampling distribution of <span class="math inline">\(\bar{x}_{Hg}\)</span>. To do this, we will sample from the approximate population of lakes, which is just many many replicated copies of our sample data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create the sampling distribution of xbar</span>
SamplingDist &lt;-<span class="st"> </span><span class="kw">do</span>(<span class="dv">10000</span>) *<span class="st"> </span><span class="kw">resample</span>(Lakes) %&gt;%<span class="st"> </span><span class="kw">summarise</span>(<span class="dt">xbar =</span> <span class="kw">mean</span>(AvgMercury))

<span class="co"># what columns does the data frame &quot;SamplingDist&quot; have?</span>
<span class="kw">head</span>(SamplingDist)</code></pre></div>
<pre><code>##        xbar
## 1 0.5111321
## 2 0.5043396
## 3 0.5252830
## 4 0.5362264
## 5 0.4992453
## 6 0.5960377</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># show a histogram of the sampling distribution of xbar</span>
<span class="kw">ggplot</span>(SamplingDist, <span class="kw">aes</span>(<span class="dt">x=</span>xbar)) +
<span class="st">  </span><span class="kw">geom_histogram</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&#39;Estimated Sampling distribution of xbar&#39;</span> )</code></pre></div>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-63-1.png" width="672" /></p>
</div>
<div id="quantile-based-confidence-intervals" class="section level2">
<h2><span class="header-section-number">3.2</span> Quantile-based Confidence Intervals</h2>
<p>In many cases we have seen, the sampling distribution of a statistic is centered on the parameter we are interested in estimating and is symmetric about that parameterThere are actually several ways to create a confidence interval from the estimated sampling distribution. The method presented here is called the “percentile” method and works when the sampling distribution is symmetric and the estimator we are using is unbiased. For example, we expect that the sample mean <span class="math inline">\(\bar{x}\)</span> should be a good estimate of the population mean <span class="math inline">\(\mu\)</span> and the sampling distribution of <span class="math inline">\(\bar{x}\)</span> should look something like the following.</p>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-64-1.png" width="672" /></p>
<p>There are two points, (call them <span class="math inline">\(L\)</span> and <span class="math inline">\(U\)</span>) where for our given sample size and population we are sampling from, where we expect that <span class="math inline">\(95\%\)</span> of the sample means to fall within. That is to say, <span class="math inline">\(L\)</span> and <span class="math inline">\(U\)</span> capture the middle <span class="math inline">\(95\%\)</span> of the sampling distribution of <span class="math inline">\(\bar{x}\)</span>.</p>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-65-1.png" width="672" /></p>
<p>These sample means are randomly distributed about the population mean <span class="math inline">\(\mu\)</span>. Given our sample data and sample mean <span class="math inline">\(\bar{x}\)</span>, we can examine how our simulated values of <span class="math inline">\(\bar{x}^{*}\)</span> vary about <span class="math inline">\(\bar{x}\)</span>. I expect that these simulated sample means <span class="math inline">\(\bar{x}^{*}\)</span> should vary about <span class="math inline">\(\bar{x}\)</span> in the same way that <span class="math inline">\(\bar{x}\)</span> values vary around <span class="math inline">\(\mu\)</span>. Below are three estimated sampling distributions that we might obtain from three different samples and their associated sample means.</p>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-66-1.png" width="672" /></p>
<p>For each possible sample, we could consider creating the estimated sampling distribution of <span class="math inline">\(\bar{X}\)</span> and calculating the <span class="math inline">\(L\)</span> and <span class="math inline">\(U\)</span> values that capture the middle <span class="math inline">\(95\%\)</span> of the estimated sampling distribution. Below are twenty samples, where we’ve calculated this interval for each sample.</p>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-67-1.png" width="672" /></p>
<p>Most of these intervals contain the true parameter <span class="math inline">\(\mu\)</span>, that we are trying to estimate. In practice, I will only take one sample and therefore will only calculate one sample mean and one interval, but I want to recognize that the method I used to produce the interval (i.e. take a random sample, calculate the mean and then the interval) will result in intervals where only <span class="math inline">\(95\%\)</span> of those intervals will contain the mean <span class="math inline">\(\mu\)</span>. Therefore, I will refer to the interval as a <span class="math inline">\(95\%\)</span> confidence interval.</p>
<p>After the sample is taken and the interval is calculated, the numbers lower and upper bounds of the confidence interval are fixed. Because <span class="math inline">\(\mu\)</span> is a constant value and the confidence interval is fixed, nothing is changing. To distinguish between a future random event and the fixed (but unknown) outcome of if I ended up with an interval that contains <span class="math inline">\(\mu\)</span> and we use the term confidence interval instead of probability interval.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create the sampling distribution of xbar</span>
SamplingDist &lt;-<span class="st"> </span><span class="kw">do</span>(<span class="dv">10000</span>) *<span class="st"> </span><span class="kw">resample</span>(Lakes) %&gt;%<span class="st"> </span><span class="kw">summarise</span>(<span class="dt">xbar=</span><span class="kw">mean</span>(AvgMercury))

<span class="co"># show a histogram of the sampling distribution of xbar</span>
<span class="kw">ggplot</span>(SamplingDist, <span class="kw">aes</span>(<span class="dt">x=</span>xbar, <span class="dt">y=</span>..density..)) +
<span class="st">  </span><span class="kw">geom_histogram</span>() +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&#39;Estimated Sampling distribution of xbar&#39;</span>)</code></pre></div>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-68-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># calculate the 95% confidence interval using middle 95% of xbars</span>
<span class="kw">quantile</span>( SamplingDist$xbar, <span class="dt">probs=</span><span class="kw">c</span>(.<span class="dv">025</span>, .<span class="dv">975</span>) )</code></pre></div>
<pre><code>##      2.5%     97.5% 
## 0.4375472 0.6211368</code></pre>
<p>There are several ways to interpret this interval.</p>
<ol style="list-style-type: decimal">
<li><p>The process used to calculate this interval (take a random sample, calculate a statistic, repeatedly resample, and take the middle <span class="math inline">\(95\%\)</span>) is a process that results in an interval that contains the parameter of interest on <span class="math inline">\(95\%\)</span> of the samples we could have collected, however we don’t know if the particular sample we collected and its resulting interval of <span class="math inline">\(\left(0.44,\,0.62\right)\)</span> is one of the intervals containing <span class="math inline">\(\mu\)</span>.</p></li>
<li><p>We are <span class="math inline">\(95\%\)</span> confident that <span class="math inline">\(\mu\)</span> is in the interval <span class="math inline">\(\left(0.44,\,0.62\right)\)</span>. This is delightfully vague and should be interpreted as a shorter version of the previous interpretation.</p></li>
<li><p>The interval <span class="math inline">\(\left(0.44,\,0.62\right)\)</span> is the set of values of <span class="math inline">\(\mu\)</span> that are consistent with the observed data at the <span class="math inline">\(0.05\)</span> threshold of statistical significance for a two-sided hypothesis test</p></li>
</ol>
<p><strong>Example</strong>: Fuel Economy</p>
<p>Suppose we have data regarding fuel economy of <span class="math inline">\(5\)</span> new vehicles of the same make and model and we wish to test if the observed fuel economy is consistent with the advertised <span class="math inline">\(31\)</span> mpg at highway speeds. We the data are</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">CarMPG &lt;-<span class="st"> </span><span class="kw">data.frame</span>( <span class="dt">ID=</span><span class="dv">1</span>:<span class="dv">5</span>, <span class="dt">mpg =</span> <span class="kw">c</span>(<span class="fl">31.8</span>, <span class="fl">32.1</span>, <span class="fl">32.5</span>, <span class="fl">30.9</span>, <span class="fl">31.3</span>) )
CarMPG %&gt;%<span class="st"> </span><span class="kw">summarise</span>( <span class="dt">xbar=</span><span class="kw">mean</span>(mpg) )</code></pre></div>
<pre><code>##    xbar
## 1 31.72</code></pre>
<p>We will use the sample mean to assess if the sample fuel efficiency is consistent with the advertised number. Because these cars could be considered a random sample of all new cars of this make, we will create the estimated sampling distribution using the bootstrap resampling of the data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">SamplingDist &lt;-<span class="st"> </span><span class="kw">do</span>(<span class="dv">10000</span>) *<span class="st"> </span><span class="kw">resample</span>(CarMPG) %&gt;%<span class="st"> </span><span class="kw">summarise</span>(<span class="dt">xbar=</span><span class="kw">mean</span>(mpg))
<span class="co"># show a histogram of the sampling distribution of xbar</span>
<span class="kw">ggplot</span>(SamplingDist, <span class="kw">aes</span>(<span class="dt">x=</span>xbar)) +
<span class="st">  </span><span class="kw">geom_histogram</span>() +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&#39;Estimated Sampling distribution of xbar&#39;</span>)</code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-70-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># calculate the 95% confidence interval using middle 95% of xbars</span>
<span class="kw">quantile</span>( SamplingDist$xbar, <span class="dt">probs=</span><span class="kw">c</span>(.<span class="dv">025</span>, .<span class="dv">975</span>) )</code></pre></div>
<pre><code>##  2.5% 97.5% 
## 31.24 32.20</code></pre>
<p>We see that the <span class="math inline">\(95\%\)</span> confidence interval is <span class="math inline">\(\left(31.2,\,32.2\right)\)</span> and does not actually contain the advertised <span class="math inline">\(31\)</span> mpg. However, I don’t think we would object to a car manufacturer selling us a car that is better than advertised.</p>
<p><strong>Example</strong>: Pulse Rate of College Students</p>
<p>In the package Lock5Data, the dataset <code>GPAGender</code> contains information taken from undergraduate students in an Introductory Statistics course. This is a convenience sample, but could be considered representative of students at that university. One of the covariates measured was the students pulse rate and we will use this to create a confidence interval for average pulse of students at that university.</p>
<p>First we’ll look at the raw data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(Lock5Data)  <span class="co"># load the package</span>
<span class="kw">data</span>(GPAGender)     <span class="co"># from the package, load the dataset</span>

<span class="co"># Now a nice histogram</span>
<span class="kw">ggplot</span>(GPAGender, <span class="kw">aes</span>(<span class="dt">x=</span>Pulse, <span class="dt">y=</span>..density..)) +
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth=</span><span class="dv">2</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&#39;Sample Data&#39;</span>)</code></pre></div>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-71-1.png" width="672" /></p>
<p>It is worth noting this was supposed to be measuring resting heart rates, but there are two students had extremely high pulse rates and six with extremely low rates. The two high values are approximately what you’d expect from someone currently engaged in moderate exercise and the low values are levels we’d expect from highly trained endurance athletes.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Summary Statistics</span>
GPAGender %&gt;%<span class="st"> </span><span class="kw">summarise</span>(<span class="dt">xbar =</span> <span class="kw">mean</span>(Pulse),
                        <span class="dt">StdDev =</span> <span class="kw">sd</span>(Pulse))</code></pre></div>
<pre><code>##       xbar   StdDev
## 1 69.90379 12.08569</code></pre>
<p>So the sample mean is <span class="math inline">\(\bar{x}=69.9\)</span> but how much should we expect our sample mean to vary from sample to sample when our sample size is <span class="math inline">\(n=343\)</span> people? We’ll estimate the sampling distribution of <span class="math inline">\(\bar{X}\)</span> using the bootstrap.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create the bootstrap replicates</span>
SampDist &lt;-<span class="st"> </span><span class="kw">do</span>(<span class="dv">10000</span>) *<span class="st"> </span><span class="kw">resample</span>(GPAGender) %&gt;%<span class="st"> </span><span class="kw">summarise</span>(<span class="dt">xbar =</span> <span class="kw">mean</span>(Pulse))

<span class="kw">ggplot</span>(SampDist, <span class="kw">aes</span>(<span class="dt">x=</span>xbar, <span class="dt">y=</span>..density..)) +
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth=</span>.<span class="dv">2</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&#39;Sampling Distribution of Mean(Pulse)&#39;</span>)</code></pre></div>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-73-1.png" width="672" /></p>
<p>Just by sampling variability, we expect the sampling mean <span class="math inline">\(\bar{X}\)</span> to vary from approximately 68 to 72. The appropriate quantiles for a <span class="math inline">\(95\%\)</span> bootstrap confidence interval are actually</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">quantile</span>( SampDist$xbar, <span class="dt">probs=</span><span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>) )</code></pre></div>
<pre><code>##     2.5%    97.5% 
## 68.65015 71.18958</code></pre>
</div>
<div id="exercises-2" class="section level2">
<h2><span class="header-section-number">3.3</span> Exercises</h2>
<p>For several of these exercises, we will use data sets from the R package <code>Lock5Data</code>, which greatly contributed to the pedagogical approach of these notes. Install the package from CRAN using the RStudio point-and-click interface <code>Tools -&gt; Install Packages</code>….</p>
<ol style="list-style-type: decimal">
<li><p>Load the dataset <code>BodyTemp50</code> from the Lock5Data package. This is a dataset of 50 healthy adults. Unfortunately the documentation doesn’t give how the data was collected, but for this problem we’ll assume that it is a representative sample of healthy US adults.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(Lock5Data)
<span class="kw">data</span>( BodyTemp50 )
?BodyTemp50</code></pre></div>
<p>One of the columns of this dataset is the Pulse of the 50 data points, which is the number of heartbeats per minute.</p>
<ol style="list-style-type: lower-alpha">
<li>Create a histogram of the observed pulse values. Comment on the graph and aspects of the graph that might be of scientific interest.</li>
<li>Calculate the sample mean <span class="math inline">\(\bar{x}\)</span> and sample standard deviation <span class="math inline">\(s\)</span> of the pulses.</li>
<li>Create a dataset of 10000 bootstrap replicates of <span class="math inline">\(\bar{x}^{*}\)</span>.</li>
<li>Create a histogram of the bootstrap replicates. Calculate the mean and standard deviation of this distribution. Notice that the standard deviation of the distribution is often called the Standard Error of <span class="math inline">\(\bar{x}\)</span> and we’ll denote it as <span class="math inline">\(\sigma_{\bar{x}}\)</span>.</li>
<li>Using the bootstrap replicates, create a 95% confidence interval for <span class="math inline">\(\mu\)</span> , the average adult heart rate.</li>
<li>Calculate the interval <span class="math display">\[\left(\bar{x}-2\cdot\hat{\sigma}_{\bar{x}}\,,\,\;\;\bar{x}+2\cdot\hat{\sigma}_{\bar{x}}\right)\]</span> and comment on its similarity to the interval you calculated in part (e).</li>
</ol></li>
<li>Load the dataset <code>EmployedACS</code> from the <code>Lock5Data</code> package. This is a dataset drawn from American Community Survey results which is conducted monthly by the US Census Bureau and should be representative of US workers. The column <code>HoursWk</code> represents the number of hours worked per week.
<ol style="list-style-type: lower-alpha">
<li>Create a histogram of the observed hours worked. Comment on the graph and aspects of the graph that might be of scientific interest.</li>
<li>Calculate the sample mean <span class="math inline">\(\bar{x}\)</span> and sample standard deviation <span class="math inline">\(s\)</span> of the worked hours per week.</li>
<li>Create a dataset of 10000 bootstrap replicates of <span class="math inline">\(\bar{x}^{*}\)</span>.</li>
<li>Create a histogram of the bootstrap replicates. Calculate the mean and standard deviation of this distribution. Notice that the standard deviation of the distribution is often called the Standard Error of <span class="math inline">\(\bar{x}\)</span> and we’ll denote it as <span class="math inline">\(\sigma_{\bar{x}}\)</span>.</li>
<li>Using the bootstrap replicates, create a 95% confidence interval for <span class="math inline">\(\mu\)</span>, the average worked hours per week.</li>
<li>Calculate the interval <span class="math display">\[\left(\bar{x}-2\cdot\hat{\sigma}_{\bar{x}}\,,\,\;\;\bar{x}+2\cdot\hat{\sigma}_{\bar{x}}\right)\]</span> and comment on its similarity to the interval you calculated in part (e).</li>
</ol></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="2-probability.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="4-sampling-distribution-of-barx.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dereksonderegger/STA_570_Book/raw/master/03_Bootstrapping.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
