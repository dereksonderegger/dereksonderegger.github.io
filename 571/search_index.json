[
["9-two-way-anova.html", "Chapter 9 Two-way ANOVA 9.1 Orthogonality 9.2 Main Effects Model 9.3 Interaction Model", " Chapter 9 Two-way ANOVA # Load my usual packages library(MASS) # for the boxcox function library(faraway) library(ggplot2) library(dplyr) library(ggfortify) library(multcompView) Given a response that is predicted by two different categorical variables. Suppose we denote the levels of the first factor as \\(\\alpha_{i}\\) and has \\(I\\) levels. The second factor has levels \\(\\beta_{j}\\) and has \\(J\\) levels. As usual we let \\(\\epsilon_{ijk}\\stackrel{iid}{\\sim}N\\left(0,\\sigma^{2}\\right)\\), and we wish to fit the model \\[y_{ijk}=\\mu+\\alpha_{i}+\\beta_{j}+\\epsilon_{ijk}\\] which has the main effects of each covariate or possibly the model with the interaction \\[y_{ijk}=\\mu+\\alpha_{i}+\\beta_{j}+\\left(\\alpha\\beta\\right)_{ij}+\\epsilon_{ijk}\\] To consider what an interaction term might mean consider the role of temperature and humidity on the amount of fungal growth. You might expect to see data similar to this (where the numbers represent some sort of measure of fungal growth): 5% 30% 60% 90% 2C 2 4 8 16 Temperature 10C 3 9 27 81 30C 4 16 64 256 In this case we see that increased humidity increases the amount of fungal growth, but the amount of increase depends on the temperature. At 2 C, the increase is humidity increases are significant, but at 10 C the increases are larger, and at 30 C the increases are larger yet. The effect of changing from one humidity level to the next depends on which temperature level we are at. This change in effect of humidity is an interaction effect. A memorable example is that chocolate by itself is good. Strawberries by themselves are also good. But the combination of chocolate and strawberries is a delight greater than the sum of the individual treats. We can look at a graph of the Humidity and Temperature vs the Response and see the effect of increasing humidity changes based on the temperature level. Just as in the ANCOVA model, the interaction manifested itself in non-parallel slopes, the interaction manifests itself in non-parallel slopes when I connect the dots across the factor levels. Unfortunately the presence of a significant interaction term in the model makes interpretation difficult, but examining the interaction plots can be quite helpful in understanding the effects. Notice in this example, we 3 levels of temperature and 4 levels of humidity for a total of 12 different possible treatment combinations. In general I will refer to these combinations as cells. 9.1 Orthogonality When designing an experiment, I want to make sure than none of my covariates are confounded with each other and I’d also like for them to not be correlated. Consider the following three experimental designs, where the number in each bin is the number of subjects of that type. I am interested in testing 2 different drugs and studying its effect on heart disease within the gender groups. Design 1 Males Females Design 2 Males Females Treatment A 0 10 Treatment A 1 9 Treatment B 6 0 Treatment B 5 1 \\(\\vspace{.2cm}\\) Design 3 Males Females Design 4 Males Females Treatment A 3 5 Treatment A 4 4 Treatment B 3 5 Treatment B 4 4 This design is very bad. Because we have no males taking drug 1, and no females taking drug 2, we can’t say if any observed differences are due to the effect of drug 1 versus 2, or gender. When this situation happens, we say that the gender effect is confounded with the drug effect. This design is not much better. Because we only have one observation in the Male-Drug 1 group, any inference we make about the effect of drug 1 on males is based on one observation. In general that is a bad idea. Design 3 is better than the previous 2 because it evenly distributes the males and females among the two drug categories. However, it seems wasteful to have more females than males because estimating average of the male groups, I only have 6 observations while I have 10 females. This is the ideal design, with equal numbers of observations in each gender-drug group. Designs 3 and 4 are good because the correlation among my predictors is 0. In design 1, the drug covariate is perfectly correlated to the gender covariate. The correlation is less in design 2, but is zero in designs 3 and 4.We could show this by calculating the design matrix for each design and calculating the correlation coefficients between each of pairs of columns. Having an orthogonal design with equal numbers of observations in each group has many nice ramifications. Most importantly, with an orthogonal design, the interpretation of parameter is not dependent on what other factors are in the model. Balanced designs are also usually optimal in the sense that the variances of \\(\\hat{\\boldsymbol{\\beta}}\\) are as small as possible given the number of observations we have (barring any other a priori information). 9.2 Main Effects Model In the one factor ANOVA case, the additional degrees of freedom used by adding a factor with \\(I\\) levels was \\(I-1\\). In the case that we consider two factors with the first factor having \\(I\\) levels and the second factor having \\(J\\) levels, then model \\[y_{ijk}=\\mu+\\alpha_{i}+\\beta_{j}+\\epsilon_{ijk}\\] adds \\((I-1)+(J-1)\\) parameters to the model because both \\(\\alpha_{1}=\\beta_{1}=0\\). The intercept term, \\(\\mu\\) is the reference point for all the other parameters. This is the expected value for an observation in the first level of factor 1 and the first level of factor two. \\(\\alpha_{i}\\) is the amount you expect the response to increase when changing from factor 1 level 1, to factor 1 level i (while the second factor is held constant). \\(\\beta_{j}\\) is the amount you expect the response to increase when changing from factor 2 level 1 to factor 2 level j (while the first factor is held constant). Referring back to the fungus example, let the \\(\\alpha_{i}\\) values be associated with changes in humidity and \\(\\beta_{j}\\) values be associated with changes in temperature levels. Then the expected value of each treatment combination is 5% 30% 60% 90% 2C \\(\\mu+0+0\\) \\(\\mu+\\alpha_2+0\\) \\(\\mu+\\alpha_3+0\\) \\(\\mu+\\alpha_4+0\\) 10C \\(\\mu+0+\\beta_2\\) \\(\\mu+\\alpha_2+\\beta_2\\) \\(\\mu+\\alpha_3+\\beta_2\\) \\(\\mu+\\alpha_4+\\beta_2\\) 30C \\(\\mu+0+\\beta_3\\) \\(\\mu+\\alpha_2+\\beta_3\\) \\(\\mu+\\alpha_3+\\beta_3\\) \\(\\mu+\\alpha_4+\\beta_3\\) 9.2.1 Example - Fruit Trees An experiment was conducted to determine the effects of four different pesticides on the yield of fruit from three different varieties of a citrus tree. Eight trees of each variety were randomly selected from an orchard. The four pesticides were randomly assigned to two trees of each variety and applications were made according to recommended levels. Yields of fruit (in bushels) were obtained after the test period. Critically notice that we have equal number of observations for each treatment combination. # Typing the data in by hand because I got this example from a really old text book... Pesticide &lt;- factor(c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;)) Variety &lt;- factor(c(&#39;1&#39;,&#39;2&#39;,&#39;3&#39;)) fruit &lt;- data.frame( expand.grid(rep=1:2, Pest=Pesticide, Var=Variety) ) fruit$Yield &lt;- c(49,39,50,55,43,38,53,48,55,41,67,58,53,42,85,73,66,68,85,92,69,62,85,99) The first thing to do (as always) is to look at our data ggplot(fruit, aes(x=Pest, color=Var, y=Yield, shape=Var)) + geom_point(size=5) The first thing we notice is that pesticides B and D seem to be better than the others and that variety 3 seems to be the best producer. The effect of pesticide treatment seems consistent between varieties, so we don’t expect that the interaction effect will be significant. We next fit a linear model and look at the diagnostic plots. m3 &lt;- lm(Yield ~ Var + Pest, data=fruit) autoplot(m3, which=1:2) There might be a little curvature in the fitted vs residuals, but because we can’t fit a polynomial to a categorical variable, and the QQ-plot looks good, we’ll ignore it for now and eventually consider an interaction term. Just for fun, we can examine the smaller models with just Variety or Pesticide. m1 &lt;- lm(Yield ~ Var, data=fruit) m2 &lt;- lm(Yield ~ Pest, data=fruit) m3 &lt;- lm(Yield ~ Var + Pest, data=fruit) summary(m1)$coef %&gt;% round(digits=3) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 46.875 4.359 10.754 0.000 ## Var2 12.375 6.164 2.008 0.058 ## Var3 31.375 6.164 5.090 0.000 summary(m2)$coef %&gt;% round(digits=3) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 53.000 6.429 8.243 0.000 ## PestB 14.833 9.093 1.631 0.118 ## PestC -1.833 9.093 -0.202 0.842 ## PestD 20.833 9.093 2.291 0.033 summary(m3)$coef %&gt;% round(digits=3) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 38.417 3.660 10.497 0.000 ## Var2 12.375 3.660 3.381 0.003 ## Var3 31.375 3.660 8.573 0.000 ## PestB 14.833 4.226 3.510 0.003 ## PestC -1.833 4.226 -0.434 0.670 ## PestD 20.833 4.226 4.930 0.000 Notice that the affects for Variety and Pesticide are the same whether or not the other is in the model. This is due to the orthogonal design of the experiment and makes it much easier to interpret the main effects of Variety and Pesticide. 9.2.2 ANOVA Table Most statistical software will produce an analysis of variance table when fitting a two-way ANOVA. This table is very similar to the analysis of variance table we have seen in the one-way ANOVA, but has several rows which correspond to the additional factors added to the model. Consider the two-way ANOVA with factors \\(A\\) and \\(B\\) which have levels \\(I\\) and \\(J\\) discrete levels respectively. For convenience let \\(RSS_{1}\\) is the residual sum of squares of the intercept-only model, and \\(RSS_{A}\\) be the residual sum of squares for the model with just the main effect of factor \\(A\\), and \\(RSS_{A+B}\\) be the residual sum of squares of the model with both main effects. Finally assume that we have a total of \\(n\\) observations. The ANOVA table for this model is as follows: Source df Sum of Sq (SS) Mean Sq F p-value A \\(df_A=I-1\\) \\(SS_A = RSS_1 - RSS_A\\) \\(MS_A = SS_A / df_A\\) \\(MS_A / MSE\\) \\(P\\left( F_{df_A, df_e} &gt; F_A \\right)\\) B \\(df_B=J-1\\) \\(SS_B = RSS_A - RSS_{A+B}\\) \\(MS_B = SS_B / df_B\\) \\(MS_B / MSE\\) \\(P\\left( F_{df_B, df_e} &gt; F_B \\right)\\) Error \\(df_e=n-I-J+1\\) \\(RSS_{A+B}\\) \\(MSE = RSS_{A+B} / df_e\\) Note, if the table is cut off, you can change decrease your font size and have it all show up… This arrangement of the ANOVA table is referred to as “Type I” sum of squares. We can examine this table in the fruit trees example using the anova() command but just passing a single model. m4 &lt;- lm(Yield ~ Var + Pest, data=fruit) anova( m4 ) ## Analysis of Variance Table ## ## Response: Yield ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Var 2 3996.1 1998.04 37.292 3.969e-07 *** ## Pest 3 2227.5 742.49 13.858 6.310e-05 *** ## Residuals 18 964.4 53.58 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We might think that this is the same as fitting three nested models and running an F-test on each successive pairs of models, but it isn’t. While both will give the same Sums of Squares, the F statistics are different because the MSE of the complex model is different. In particular, the F-statistics are larger and thus the p-values are smaller for detecting significant effects. m1 &lt;- lm(Yield ~ 1, data=fruit) m2 &lt;- lm(Yield ~ Var, data=fruit) m3 &lt;- lm(Yield ~ Var + Pest, data=fruit) anova( m1, m2 ) ## Analysis of Variance Table ## ## Model 1: Yield ~ 1 ## Model 2: Yield ~ Var ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 23 7188.0 ## 2 21 3191.9 2 3996.1 13.146 0.0001987 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova( m2, m3 ) ## Analysis of Variance Table ## ## Model 1: Yield ~ Var ## Model 2: Yield ~ Var + Pest ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 21 3191.9 ## 2 18 964.4 3 2227.5 13.858 6.31e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 9.2.3 Estimating Contrasts As in the one-way ANOVA, we are interested in which factor levels differ. For example, we might suspect that it makes sense to group pesticides B and D together and claim that they are better than the group of A and C. Just as we did in the one-way ANOVA model, this is such a common thing to do that there is an easy way to do this, using TukeyHSD, which requires us coerce our model output to an aov object. We could specify all of the contrasts by hand and use glht() in the multcomp package to calculate all of the contrasts, but using TukeyHSD will be simpler. m3 &lt;- lm(Yield ~ Var + Pest, data=fruit) TukeyHSD(aov(m3)) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = m3) ## ## $Var ## diff lwr upr p adj ## 2-1 12.375 3.034405 21.71559 0.0088734 ## 3-1 31.375 22.034405 40.71559 0.0000003 ## 3-2 19.000 9.659405 28.34059 0.0001735 ## ## $Pest ## diff lwr upr p adj ## B-A 14.833333 2.889267 26.777399 0.0121713 ## C-A -1.833333 -13.777399 10.110733 0.9718552 ## D-A 20.833333 8.889267 32.777399 0.0005728 ## C-B -16.666667 -28.610733 -4.722601 0.0047986 ## D-B 6.000000 -5.944066 17.944066 0.5037838 ## D-C 22.666667 10.722601 34.610733 0.0002286 The output from the TukeyHSD is quite useful, but it would be nice to generate the data frame indicating group differences similar to what we did in the one-way ANOVA. We will use the exact same function we had before: #&#39; Create a data frame with significant groupings #&#39; #&#39; This function runs TukeyHSD on the input model and then creates a data frame #&#39; with a column for the factor and a second for the Significance Group #&#39; #&#39; @param model The output of a lm() or aov() call that can be coerced to an aov object. #&#39; @param variable The variable of interest. #&#39; @output A data frame with a column for factor and another for the signicance group. make_TukeyHSD_letters &lt;- function(model, variable){ Tukey &lt;- TukeyHSD(aov(model))[[variable]] temp &lt;- Tukey[,&#39;p adj&#39;] %&gt;% vec2mat() %&gt;% multcompLetters() out &lt;- data.frame(group = names(temp$Letters), SigGroup=temp$Letters) colnames(out)[1] &lt;- variable out } make_TukeyHSD_letters( m3, &#39;Var&#39;) ## Var SigGroup ## 2 2 a ## 3 3 b ## 1 1 c make_TukeyHSD_letters( m3, &#39;Pest&#39;) ## Pest SigGroup ## B B a ## C C b ## D D a ## A A b So we see that each variety is significantly different from all the others and among the pesticides, \\(A\\) and \\(C\\) are indistigishable as are \\(B\\) and \\(D\\), but there is a difference between the \\(A,C\\) and \\(B,D\\) groups. 9.3 Interaction Model When the model contains the interaction of the two factors, our model is written as \\[y_{ijk}=\\mu+\\alpha_{i}+\\beta_{j}+\\left(\\alpha\\beta\\right)_{ij}+\\epsilon_{ijk}\\] Interpreting effects effects can be very tricky. Under the interaction, the effect of changing from factor 1 level 1 to factor 1 level \\(i\\) depends on what level of factor 2 is. In essence, we are fitting a model that allows each of the \\(I\\times J\\) cells in my model to vary independently. As such, the model has a total of \\(I\\times J\\) parameters but because the model without interactions had \\(1+(I-1)+(J-1)\\) terms in it, the interaction is adding \\(df_{AB}\\) parameters. We can solve for this via: \\[\\begin{aligned} I\\times J &amp;= 1+(I-1)+(J-1)+df_{AB} \\\\ I\\times J &amp;= I+J-1+df_{AB} \\\\ IJ-I-J &amp;= -1+df_{AB} \\\\ I(J-1)-J &amp;= -1+df_{AB} \\\\ I(J-1)-J+1 &amp;= df_{AB} \\\\ I(J-1)-(J-1) &amp;= df_{AB} \\\\ (I-1)(J-1) &amp;= df_{AB} \\end{aligned}\\] This makes sense because the first factor added \\((I-1)\\) columns to the design matrix and an interaction with a continuous covariate just multiplied the columns of the factor by the single column of the continuous covariate. Creating an interaction of two factors multiplies each column of the first factor by all the columns defined by the second factor. The expected value of the \\(ij\\) combination is \\(\\mu+\\alpha_{i}+\\beta_{j}+\\left(\\alpha\\beta\\right)_{ij}\\). Returning to our fungus example, the expected means for each treatment under the model with main effects and the interaction is 5% 30% 60% 90% 2C \\(\\mu+0+0+0\\) \\(\\mu+\\alpha_2+0+0\\) \\(\\mu+\\alpha_3+0+0\\) \\(\\mu+\\alpha_4+0+0\\) 10C \\(\\mu+0+\\beta_2+0\\) \\(\\mu+\\alpha_2+\\beta_2+\\left(\\alpha\\beta\\right)_{22}\\) \\(\\mu+\\alpha_3+\\beta_2+\\left(\\alpha\\beta\\right)_{32}\\) \\(\\mu+\\alpha_4+\\beta_2+\\left(\\alpha\\beta\\right)_{42}\\) 30C \\(\\mu+0+\\beta_3+0\\) \\(\\mu+\\alpha_2+\\beta_3+\\left(\\alpha\\beta\\right)_{23}\\) \\(\\mu+\\alpha_3+\\beta_2+\\left(\\alpha\\beta\\right)_{33}\\) \\(\\mu+\\alpha_4+\\beta_2+\\left(\\alpha\\beta\\right)_{43}\\) Notice that we have added \\(6=3\\cdot2=\\left(4-1\\right)\\left(3-1\\right)=\\left(I-1\\right)\\left(J-1\\right)\\) interaction parameters \\(\\left(\\alpha\\beta\\right)_{ij}\\) to the main effects only model. The interaction model has \\(p=12\\) parameters, one for each cell in my treatment array. In general it is hard to interpret the meaning of \\(\\alpha_{i}\\), \\(\\beta_{j}\\), and \\(\\left(\\alpha\\beta\\right)_{ij}\\) and the best way to make sense of them is to look at the interaction plots. 9.3.1 ANOVA Table Most statistical software will produce an analysis of variance table when fitting a two-way ANOVA. This table is very similar to the analysis of variance table we have seen in the one-way ANOVA, but has several rows which correspond to the additional factors added to the model. Consider the two-way ANOVA with factors \\(A\\) and \\(B\\) which have levels \\(I\\) and \\(J\\) discrete levels respectively. For convenience let \\(RSS_{1}\\) be the residual sum of squares of the intercept-only model, and \\(RSS_{A}\\) be the residual sum of squares for the model with just the main effect of factor \\(A\\). Likewise \\(RSS_{A+B}\\) and \\(RSS_{A*B}\\) shall be the residual sum of squares of the model with just the main effects and the model with main effects and the interaction. Finally assume that we have a total of \\(n\\) observations. The ANOVA table for this model is as follows: df Sum Sq (SS) MS F \\(Pr(\\ge F)\\) A \\(df_A=I-1\\) \\(SS_A = RSS_1 - RSS_A\\) \\(MS_A = SS_A/df_A\\) \\(MS_A / MSE\\) \\(Pr(F_{df_A,df_{\\epsilon}} \\ge F_A\\) B \\(df_B=J-1\\) \\(SS_B = RSS_A - RSS_{A+B}\\) \\(MS_B = SS_B/df_B\\) \\(MS_B / MSE\\) \\(Pr(F_{df_B,df_{\\epsilon}} \\ge F_B\\) AB \\(df_{AB}=(I-1)(J-1)\\) \\(SS_{A*B} = RSS_{A*B}-RSS_{A+B}\\) \\(MS_{AB} = SS_{AB} / df_{AB}\\) \\(MS_{AB} MSE\\) \\(Pr(F_{df_{AB},df_{\\epsilon}} \\ge F_{AB}\\) Error \\(df_{\\epsilon}=n-IJ\\) \\(RSS_{A*B}\\) \\(MSE = RSS_{A*B} / df_{\\epsilon}\\) This arrangement of the ANOVA table is referred to as “Type I” sum of squares. Type III sums of squares are the difference between the full interaction model and the model removing each parameter group, even when it doesn’t make sense. For example in the Type III table, \\(SS_{A}=RSS_{B+A:B}-RSS_{A*B}\\). There is an intermediate form of the sums of squares called Type II, that when removing a main effect also removes the higher order interaction. In the case of balanced (orthogonal) designs, there is no difference between the different types, but for non-balanced designs, the numbers will change. To access these other types of sums of squares, use the Anova() function in the package car. 9.3.2 Example - Fruit Trees (continued) We next consider whether or not to include the interaction term to the fruit tree model. We fit the model with the interaction and then graph the results. m4 &lt;- lm(Yield ~ Var * Pest, data=fruit) fruit$y.hat &lt;- predict(m4) ggplot(fruit, aes(x=Pest, color=Var, shape=Var, y=Yield)) + geom_point(size=5) + geom_line(aes(y=y.hat, x=as.integer(Pest))) All of the line segments are close to parallel so, we don’t expect the interaction to be significant. anova( m4 ) ## Analysis of Variance Table ## ## Response: Yield ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Var 2 3996.1 1998.04 47.2443 2.048e-06 *** ## Pest 3 2227.5 742.49 17.5563 0.0001098 *** ## Var:Pest 6 456.9 76.15 1.8007 0.1816844 ## Residuals 12 507.5 42.29 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Examining the ANOVA table, we see that the interaction effect is not significant and we will stay with simpler model Yield~Var+Pest. 9.3.3 Example - Warpbreaks This data set looks at the number of breaks that occur in two different types of wool under three different levels of tension (low, medium, and high). The fewer number of breaks, the better. As always, the first thing we do is look at the data. In this case, it looks like the number of breaks decreases with increasing tension and perhaps wool B has fewer breaks than wool A. library(ggplot2) library(faraway) data(warpbreaks) ggplot(warpbreaks, aes(x=tension, y=breaks, color=wool, shape=wool), size=2) + geom_boxplot() + geom_point(position=position_dodge(width=.35)) # offset the wool groups We next fit our linear model and examine the diagnostic plots. model &lt;- lm(breaks ~ tension + wool, data=warpbreaks) autoplot(model, which=c(1,2)) The residuals vs fitted values plot is a little worrisome and appears to be an issue with non-constant variance, but the normality assumption looks good. We’ll check for a Box-Cox transformation next. boxcox(model) This suggests we should make a log transformation, though because the confidence interval is quite wide we might consider if the increased difficulty in interpretation makes sufficient progress towards making the data meet the model assumptions.. The diagnostic plots of the resulting model look better for the constant variance assumption, but the normality is now a worse off. Because the Central Limit Theorem helps deal with the normality question, I’d rather stabilize the variance at the cost of the normality. model.1 &lt;- lm(log(breaks) ~ tension + wool, data=warpbreaks) autoplot(model.1, which=c(1,2)) Next we’ll fit the interaction model and check the diagnostic plots. The diagnostic plots look good and this appears to be a legitimate model. model.2 &lt;- lm(log(breaks) ~ tension * wool, data=warpbreaks) autoplot(model.2, which=c(1,2)) Then we’ll do an F-test to see if it is a better model than the main effects model. The p-value is marginally significant, so we’ll keep the interaction in the model, but recognize that it is a weak interaction. anova(model.1, model.2) ## Analysis of Variance Table ## ## Model 1: log(breaks) ~ tension + wool ## Model 2: log(breaks) ~ tension * wool ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 50 7.6270 ## 2 48 6.7138 2 0.91315 3.2642 0.04686 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Next we look at the effect of the interaction and the easiest way to do this is to look at the interaction plot. This plot shows the raw data and connects lines to the cell mean of each factor combination. warpbreaks$logy.hat &lt;- predict(model.2) ggplot(warpbreaks, aes(x=tension, y=log(breaks), color=wool, shape=wool)) + geom_point() + geom_line(aes(y=logy.hat, x=as.integer(tension))) We can see that it appears that wool A has a decrease in breaks between low and medium tension, while wool B has a decrease in breaks between medium and high. It is actually quite difficult to see this interaction when we examine the model coefficients. summary(model.2) ## ## Call: ## lm(formula = log(breaks) ~ tension * wool, data = warpbreaks) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.81504 -0.27885 0.04042 0.27319 0.64358 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.7179 0.1247 29.824 &lt; 2e-16 *** ## tensionM -0.6012 0.1763 -3.410 0.00133 ** ## tensionH -0.6003 0.1763 -3.405 0.00134 ** ## woolB -0.4356 0.1763 -2.471 0.01709 * ## tensionM:woolB 0.6281 0.2493 2.519 0.01514 * ## tensionH:woolB 0.2221 0.2493 0.891 0.37749 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.374 on 48 degrees of freedom ## Multiple R-squared: 0.3363, Adjusted R-squared: 0.2672 ## F-statistic: 4.864 on 5 and 48 DF, p-value: 0.001116 To test if there is a statistically significant difference between medium and high tensions for wool type B, we really need to test the following hypothesis: \\[\\begin{aligned} H_{0}:\\;\\left(\\mu+\\alpha_{2}+\\beta_{2}+\\left(\\alpha\\beta\\right)_{22}\\right)-\\left(\\mu+\\alpha_{3}+\\beta_{2}+\\left(\\alpha\\beta\\right)_{32}\\right) &amp; = 0 \\\\ H_{a}:\\;\\left(\\mu+\\alpha_{2}+\\beta_{2}+\\left(\\alpha\\beta\\right)_{22}\\right)-\\left(\\mu+\\alpha_{3}+\\beta_{2}+\\left(\\alpha\\beta\\right)_{32}\\right) &amp;\\ne 0 \\end{aligned}\\] This test reduces to testing if \\(\\alpha_{2}-\\alpha_{3}+\\left(\\alpha\\beta\\right)_{22}-\\left(\\alpha\\beta\\right)_{23}=0\\). Calculating this difference from the estimated values of the summery table we have \\(-.6012+.6003+.6281-.2221=0.4051\\), we don’t know if that is significantly different than zero. In the main effects model, we were able to read off the necessary test using Tukey’s HSD. Fortunately, we can do the same thing here. In this case, we’ll look at the interactions piece of the TukeyHSD command. In this case, we find the test H:B - M:B in the last row of the interactions. TukeyHSD( aov(model.2) ) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = model.2) ## ## $tension ## diff lwr upr p adj ## M-L -0.2871237 -0.5886239 0.01437636 0.0649432 ## H-L -0.4892747 -0.7907748 -0.18777463 0.0007948 ## H-M -0.2021510 -0.5036511 0.09934912 0.2465032 ## ## $wool ## diff lwr upr p adj ## B-A -0.1521536 -0.3568127 0.05250558 0.1415114 ## ## $`tension:wool` ## diff lwr upr p adj ## M:A-L:A -0.6011957092 -1.1244431 -0.07794828 0.0157632 ## H:A-L:A -0.6003226799 -1.1235701 -0.07707525 0.0159794 ## L:B-L:A -0.4355668365 -0.9588143 0.08768059 0.1534713 ## M:B-L:A -0.4086186238 -0.9318661 0.11462881 0.2071300 ## H:B-L:A -0.8137936293 -1.3370411 -0.29054620 0.0004035 ## H:A-M:A 0.0008730293 -0.5223744 0.52412046 1.0000000 ## L:B-M:A 0.1656288727 -0.3576186 0.68887630 0.9341161 ## M:B-M:A 0.1925770854 -0.3306703 0.71582452 0.8820529 ## H:B-M:A -0.2125979201 -0.7358454 0.31064951 0.8318529 ## L:B-H:A 0.1647558434 -0.3584916 0.68800327 0.9354994 ## M:B-H:A 0.1917040562 -0.3315434 0.71495149 0.8840239 ## H:B-H:A -0.2134709494 -0.7367184 0.30977648 0.8294547 ## M:B-L:B 0.0269482127 -0.4962992 0.55019564 0.9999876 ## H:B-L:B -0.3782267929 -0.9014742 0.14502064 0.2822984 ## H:B-M:B -0.4051750056 -0.9284224 0.11807242 0.2148594 So TukeyHSD gives us all the tests comparing the cell means, but what are those main effects testing? In the case where our experiment is balanced with equal numbers of observations in each treatment cell, we can interpret these differences as follows. Knowing that each cell in our table has a different estimated mean, we could consider the average of all the type A cells as the typical wool A. Likewise we could average all the cell means for the wool B cells. Then we could look at the difference between those two averages. In the balanced design, this is equivalent to removing the tension term from the model and just looking at the difference between the average log number of breaks. Using TukeyHSD, we can see the wool effect difference between types B and A is \\(-0.1522\\). We can calculate the mean number of log breaks for each wool type and take the difference by the following: warpbreaks %&gt;% group_by(wool) %&gt;% summarise( wool.means = mean(log(breaks)) ) %&gt;% summarise( diff(wool.means) ) ## # A tibble: 1 x 1 ## diff(wool.means) ## &lt;dbl&gt; ## 1 -0.1521536 In the unbalanced case taking the average of the cell means produces a different answer than taking the average of the data. It isn’t clear which is preferred and TukeyHSD produces a result that is between those two methods. "]
]
