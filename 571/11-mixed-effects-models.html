<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Statistical Methods II</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="The second semester of an Intro Stats course designed for graduate students in Biology, Forestry, Ecology, etc.">
  <meta name="generator" content="bookdown 0.1.5 and GitBook 2.6.7">

  <meta property="og:title" content="Statistical Methods II" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The second semester of an Intro Stats course designed for graduate students in Biology, Forestry, Ecology, etc." />
  <meta name="github-repo" content="dereksonderegger/STA_571_Book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Statistical Methods II" />
  
  <meta name="twitter:description" content="The second semester of an Intro Stats course designed for graduate students in Biology, Forestry, Ecology, etc." />
  

<meta name="author" content="Derek L. Sonderegger">

<meta name="date" content="2016-10-25">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="10-block-designs.html">


<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Methods II</a></li>
<li><a href="https://dereksonderegger.github.io/571/Statistical_Methods_II.pdf" target="blank">PDF version</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Matrix Theory</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#types-of-matrices"><i class="fa fa-check"></i><b>1.1</b> Types of Matrices</a><ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#scalars"><i class="fa fa-check"></i><b>1.1.1</b> Scalars</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#vectors"><i class="fa fa-check"></i><b>1.1.2</b> Vectors</a></li>
<li class="chapter" data-level="1.1.3" data-path="index.html"><a href="index.html#matrix"><i class="fa fa-check"></i><b>1.1.3</b> Matrix</a></li>
<li class="chapter" data-level="1.1.4" data-path="index.html"><a href="index.html#square-matrices"><i class="fa fa-check"></i><b>1.1.4</b> Square Matrices</a></li>
<li class="chapter" data-level="1.1.5" data-path="index.html"><a href="index.html#symmetric-matrices"><i class="fa fa-check"></i><b>1.1.5</b> Symmetric Matrices</a></li>
<li class="chapter" data-level="1.1.6" data-path="index.html"><a href="index.html#diagonal-matrices"><i class="fa fa-check"></i><b>1.1.6</b> Diagonal Matrices</a></li>
<li class="chapter" data-level="1.1.7" data-path="index.html"><a href="index.html#identity-matrices"><i class="fa fa-check"></i><b>1.1.7</b> Identity Matrices</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#operations-on-matrices"><i class="fa fa-check"></i><b>1.2</b> Operations on Matrices</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#transpose"><i class="fa fa-check"></i><b>1.2.1</b> Transpose</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#addition-and-subtraction"><i class="fa fa-check"></i><b>1.2.2</b> Addition and Subtraction</a></li>
<li class="chapter" data-level="1.2.3" data-path="index.html"><a href="index.html#multiplication"><i class="fa fa-check"></i><b>1.2.3</b> Multiplication</a></li>
<li class="chapter" data-level="1.2.4" data-path="index.html"><a href="index.html#vector-multiplication"><i class="fa fa-check"></i><b>1.2.4</b> Vector Multiplication</a></li>
<li class="chapter" data-level="1.2.5" data-path="index.html"><a href="index.html#matrix-multiplication"><i class="fa fa-check"></i><b>1.2.5</b> Matrix Multiplication</a></li>
<li class="chapter" data-level="1.2.6" data-path="index.html"><a href="index.html#scalar-times-a-matrix"><i class="fa fa-check"></i><b>1.2.6</b> Scalar times a Matrix</a></li>
<li class="chapter" data-level="1.2.7" data-path="index.html"><a href="index.html#determinant"><i class="fa fa-check"></i><b>1.2.7</b> Determinant</a></li>
<li class="chapter" data-level="1.2.8" data-path="index.html"><a href="index.html#inverse"><i class="fa fa-check"></i><b>1.2.8</b> Inverse</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#exercises"><i class="fa fa-check"></i><b>1.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html"><i class="fa fa-check"></i><b>2</b> Parameter Estimation</a><ul>
<li class="chapter" data-level="2.1" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#simple-regression"><i class="fa fa-check"></i><b>2.1</b> Simple Regression</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#estimation-of-location-paramters"><i class="fa fa-check"></i><b>2.1.1</b> Estimation of Location Paramters</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#estimation-of-variance-parameter"><i class="fa fa-check"></i><b>2.1.2</b> Estimation of Variance Parameter</a></li>
<li class="chapter" data-level="2.1.3" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#expectation-and-variance-of-a-random-vector"><i class="fa fa-check"></i><b>2.1.3</b> Expectation and variance of a random vector</a></li>
<li class="chapter" data-level="2.1.4" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#variance-of-location-parameters"><i class="fa fa-check"></i><b>2.1.4</b> Variance of Location Parameters</a></li>
<li class="chapter" data-level="2.1.5" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#confidence-intervals-and-hypothesis-tests"><i class="fa fa-check"></i><b>2.1.5</b> Confidence intervals and hypothesis tests</a></li>
<li class="chapter" data-level="2.1.6" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#summary-of-pertinent-results"><i class="fa fa-check"></i><b>2.1.6</b> Summary of pertinent results</a></li>
<li class="chapter" data-level="2.1.7" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#an-example-in-r"><i class="fa fa-check"></i><b>2.1.7</b> An example in R</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#anova-model"><i class="fa fa-check"></i><b>2.2</b> ANOVA model</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#cell-means-representation"><i class="fa fa-check"></i><b>2.2.1</b> Cell means representation</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#offset-from-reference-group"><i class="fa fa-check"></i><b>2.2.2</b> Offset from reference group</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#exercises-1"><i class="fa fa-check"></i><b>2.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-inference.html"><a href="3-inference.html"><i class="fa fa-check"></i><b>3</b> Inference</a><ul>
<li class="chapter" data-level="3.1" data-path="3-inference.html"><a href="3-inference.html#f-tests"><i class="fa fa-check"></i><b>3.1</b> F-tests</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-inference.html"><a href="3-inference.html#theory"><i class="fa fa-check"></i><b>3.1.1</b> Theory</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-inference.html"><a href="3-inference.html#testing-all-covariates"><i class="fa fa-check"></i><b>3.1.2</b> Testing All Covariates</a></li>
<li class="chapter" data-level="3.1.3" data-path="3-inference.html"><a href="3-inference.html#testing-a-single-covariate"><i class="fa fa-check"></i><b>3.1.3</b> Testing a Single Covariate</a></li>
<li class="chapter" data-level="3.1.4" data-path="3-inference.html"><a href="3-inference.html#testing-a-subset-of-covariates"><i class="fa fa-check"></i><b>3.1.4</b> Testing a Subset of Covariates</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-inference.html"><a href="3-inference.html#confidence-intervals-for-location-parameters"><i class="fa fa-check"></i><b>3.2</b> Confidence Intervals for location parameters</a></li>
<li class="chapter" data-level="3.3" data-path="3-inference.html"><a href="3-inference.html#prediction-and-confidence-intervals-for-a-response"><i class="fa fa-check"></i><b>3.3</b> Prediction and Confidence Intervals for a response</a></li>
<li class="chapter" data-level="3.4" data-path="3-inference.html"><a href="3-inference.html#interpretation-with-correlated-covariates"><i class="fa fa-check"></i><b>3.4</b> Interpretation with Correlated Covariates</a></li>
<li class="chapter" data-level="3.5" data-path="3-inference.html"><a href="3-inference.html#exercises-2"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-analysis-of-covariance-ancova.html"><a href="4-analysis-of-covariance-ancova.html"><i class="fa fa-check"></i><b>4</b> Analysis of Covariance (ANCOVA)</a><ul>
<li class="chapter" data-level="4.1" data-path="4-analysis-of-covariance-ancova.html"><a href="4-analysis-of-covariance-ancova.html#offset-parallel-lines-aka-additive-models"><i class="fa fa-check"></i><b>4.1</b> Offset parallel Lines (aka additive models)</a></li>
<li class="chapter" data-level="4.2" data-path="4-analysis-of-covariance-ancova.html"><a href="4-analysis-of-covariance-ancova.html#lines-with-different-slopes-aka-interaction-model"><i class="fa fa-check"></i><b>4.2</b> Lines with different slopes (aka Interaction model)</a></li>
<li class="chapter" data-level="4.3" data-path="4-analysis-of-covariance-ancova.html"><a href="4-analysis-of-covariance-ancova.html#iris-example"><i class="fa fa-check"></i><b>4.3</b> Iris Example</a></li>
<li class="chapter" data-level="4.4" data-path="4-analysis-of-covariance-ancova.html"><a href="4-analysis-of-covariance-ancova.html#exercises-3"><i class="fa fa-check"></i><b>4.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-contrasts.html"><a href="5-contrasts.html"><i class="fa fa-check"></i><b>5</b> Contrasts</a><ul>
<li class="chapter" data-level="5.1" data-path="5-contrasts.html"><a href="5-contrasts.html#estimate-and-variance"><i class="fa fa-check"></i><b>5.1</b> Estimate and variance</a></li>
<li class="chapter" data-level="5.2" data-path="5-contrasts.html"><a href="5-contrasts.html#estimating-contrasts-in-r"><i class="fa fa-check"></i><b>5.2</b> Estimating contrasts in R</a><ul>
<li class="chapter" data-level="5.2.1" data-path="5-contrasts.html"><a href="5-contrasts.html#way-anova"><i class="fa fa-check"></i><b>5.2.1</b> 1-way ANOVA</a></li>
<li class="chapter" data-level="5.2.2" data-path="5-contrasts.html"><a href="5-contrasts.html#ancova-example"><i class="fa fa-check"></i><b>5.2.2</b> ANCOVA example</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-contrasts.html"><a href="5-contrasts.html#exercises-4"><i class="fa fa-check"></i><b>5.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-diagnostics-and-transformations.html"><a href="6-diagnostics-and-transformations.html"><i class="fa fa-check"></i><b>6</b> Diagnostics and Transformations</a><ul>
<li class="chapter" data-level="6.1" data-path="6-diagnostics-and-transformations.html"><a href="6-diagnostics-and-transformations.html#detecting-assumption-violations"><i class="fa fa-check"></i><b>6.1</b> Detecting Assumption Violations</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-diagnostics-and-transformations.html"><a href="6-diagnostics-and-transformations.html#measures-of-influence"><i class="fa fa-check"></i><b>6.1.1</b> Measures of Influence</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-diagnostics-and-transformations.html"><a href="6-diagnostics-and-transformations.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.1.2</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-diagnostics-and-transformations.html"><a href="6-diagnostics-and-transformations.html#transformations"><i class="fa fa-check"></i><b>6.2</b> Transformations</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-diagnostics-and-transformations.html"><a href="6-diagnostics-and-transformations.html#transforming-the-response"><i class="fa fa-check"></i><b>6.2.1</b> Transforming the Response</a></li>
<li class="chapter" data-level="6.2.2" data-path="6-diagnostics-and-transformations.html"><a href="6-diagnostics-and-transformations.html#transforming-the-predictors"><i class="fa fa-check"></i><b>6.2.2</b> Transforming the predictors</a></li>
<li class="chapter" data-level="6.2.3" data-path="6-diagnostics-and-transformations.html"><a href="6-diagnostics-and-transformations.html#interpretation-of-log-transformed-variables"><i class="fa fa-check"></i><b>6.2.3</b> Interpretation of log transformed variables</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-diagnostics-and-transformations.html"><a href="6-diagnostics-and-transformations.html#exercises-5"><i class="fa fa-check"></i><b>6.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-variable-selection.html"><a href="7-variable-selection.html"><i class="fa fa-check"></i><b>7</b> Variable Selection</a><ul>
<li class="chapter" data-level="7.1" data-path="7-variable-selection.html"><a href="7-variable-selection.html#nested-models"><i class="fa fa-check"></i><b>7.1</b> Nested Models</a></li>
<li class="chapter" data-level="7.2" data-path="7-variable-selection.html"><a href="7-variable-selection.html#testing-based-model-selection"><i class="fa fa-check"></i><b>7.2</b> Testing-Based Model Selection</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7-variable-selection.html"><a href="7-variable-selection.html#example---u.s.-life-expectancy"><i class="fa fa-check"></i><b>7.2.1</b> Example - U.S. Life Expectancy</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7-variable-selection.html"><a href="7-variable-selection.html#criterion-based-procedures"><i class="fa fa-check"></i><b>7.3</b> Criterion Based Procedures</a><ul>
<li class="chapter" data-level="7.3.1" data-path="7-variable-selection.html"><a href="7-variable-selection.html#information-criterions"><i class="fa fa-check"></i><b>7.3.1</b> Information Criterions</a></li>
<li class="chapter" data-level="7.3.2" data-path="7-variable-selection.html"><a href="7-variable-selection.html#adjusted-r-sq"><i class="fa fa-check"></i><b>7.3.2</b> Adjusted <code>R-sq</code></a></li>
<li class="chapter" data-level="7.3.3" data-path="7-variable-selection.html"><a href="7-variable-selection.html#example"><i class="fa fa-check"></i><b>7.3.3</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7-variable-selection.html"><a href="7-variable-selection.html#exercises-6"><i class="fa fa-check"></i><b>7.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-one-way-anova.html"><a href="8-one-way-anova.html"><i class="fa fa-check"></i><b>8</b> One way ANOVA</a><ul>
<li class="chapter" data-level="8.1" data-path="8-one-way-anova.html"><a href="8-one-way-anova.html#an-example"><i class="fa fa-check"></i><b>8.1</b> An Example</a></li>
<li class="chapter" data-level="8.2" data-path="8-one-way-anova.html"><a href="8-one-way-anova.html#degrees-of-freedom"><i class="fa fa-check"></i><b>8.2</b> Degrees of Freedom</a></li>
<li class="chapter" data-level="8.3" data-path="8-one-way-anova.html"><a href="8-one-way-anova.html#diagnostics"><i class="fa fa-check"></i><b>8.3</b> Diagnostics</a></li>
<li class="chapter" data-level="8.4" data-path="8-one-way-anova.html"><a href="8-one-way-anova.html#pairwise-comparisons"><i class="fa fa-check"></i><b>8.4</b> Pairwise Comparisons</a><ul>
<li class="chapter" data-level="8.4.1" data-path="8-one-way-anova.html"><a href="8-one-way-anova.html#presentation-of-results"><i class="fa fa-check"></i><b>8.4.1</b> Presentation of Results</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="8-one-way-anova.html"><a href="8-one-way-anova.html#exercises-7"><i class="fa fa-check"></i><b>8.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-two-way-anova.html"><a href="9-two-way-anova.html"><i class="fa fa-check"></i><b>9</b> Two-way ANOVA</a><ul>
<li class="chapter" data-level="9.1" data-path="9-two-way-anova.html"><a href="9-two-way-anova.html#orthogonality"><i class="fa fa-check"></i><b>9.1</b> Orthogonality</a></li>
<li class="chapter" data-level="9.2" data-path="9-two-way-anova.html"><a href="9-two-way-anova.html#main-effects-model"><i class="fa fa-check"></i><b>9.2</b> Main Effects Model</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9-two-way-anova.html"><a href="9-two-way-anova.html#example---fruit-trees"><i class="fa fa-check"></i><b>9.2.1</b> Example - Fruit Trees</a></li>
<li class="chapter" data-level="9.2.2" data-path="9-two-way-anova.html"><a href="9-two-way-anova.html#anova-table"><i class="fa fa-check"></i><b>9.2.2</b> ANOVA Table</a></li>
<li class="chapter" data-level="9.2.3" data-path="9-two-way-anova.html"><a href="9-two-way-anova.html#estimating-contrasts"><i class="fa fa-check"></i><b>9.2.3</b> Estimating Contrasts</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9-two-way-anova.html"><a href="9-two-way-anova.html#interaction-model"><i class="fa fa-check"></i><b>9.3</b> Interaction Model</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9-two-way-anova.html"><a href="9-two-way-anova.html#anova-table-1"><i class="fa fa-check"></i><b>9.3.1</b> ANOVA Table</a></li>
<li class="chapter" data-level="9.3.2" data-path="9-two-way-anova.html"><a href="9-two-way-anova.html#example---fruit-trees-continued"><i class="fa fa-check"></i><b>9.3.2</b> Example - Fruit Trees (continued)</a></li>
<li class="chapter" data-level="9.3.3" data-path="9-two-way-anova.html"><a href="9-two-way-anova.html#example---warpbreaks"><i class="fa fa-check"></i><b>9.3.3</b> Example - Warpbreaks</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9-two-way-anova.html"><a href="9-two-way-anova.html#exercises-8"><i class="fa fa-check"></i><b>9.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-block-designs.html"><a href="10-block-designs.html"><i class="fa fa-check"></i><b>10</b> Block Designs</a><ul>
<li class="chapter" data-level="10.1" data-path="10-block-designs.html"><a href="10-block-designs.html#randomized-complete-block-design-rcbd"><i class="fa fa-check"></i><b>10.1</b> Randomized Complete Block Design (RCBD)</a></li>
<li class="chapter" data-level="10.2" data-path="10-block-designs.html"><a href="10-block-designs.html#split-plot-designs"><i class="fa fa-check"></i><b>10.2</b> Split-plot designs</a></li>
<li class="chapter" data-level="10.3" data-path="10-block-designs.html"><a href="10-block-designs.html#exercises-9"><i class="fa fa-check"></i><b>10.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html"><i class="fa fa-check"></i><b>11</b> Mixed Effects Models</a><ul>
<li class="chapter" data-level="11.1" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html#review-of-maximum-likelihood-methods"><i class="fa fa-check"></i><b>11.1</b> Review of Maximum Likelihood Methods</a></li>
<li class="chapter" data-level="11.2" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html#way-anova-with-a-random-effect"><i class="fa fa-check"></i><b>11.2</b> 1-way ANOVA with a random effect</a></li>
<li class="chapter" data-level="11.3" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html#blocks-as-random-variables"><i class="fa fa-check"></i><b>11.3</b> Blocks as Random Variables</a></li>
<li class="chapter" data-level="11.4" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html#nested-effects"><i class="fa fa-check"></i><b>11.4</b> Nested Effects</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Methods II</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mixed-effects-models" class="section level1">
<h1><span class="header-section-number">Chapter 11</span> Mixed Effects Models</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(faraway)
<span class="kw">library</span>(lme4)
<span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(lsmeans)</code></pre></div>
<p>The assumption of independent observations is often not supported and dependent data arises in a wide variety of situations. The dependency structure could be very simple such as rabbits within a litter being correlated and the litters being independent. More complex hierarchies of correlation are possible. For example we might expect voters in a particular part of town (called a precinct) to vote similarly, and particular districts in a state tend to vote similarly as well, which might result in a precinct / district / state hierarchy of correlation.</p>
<p>Many of the designs mentioned in the Block Designs section could be similarly modeled using Mixed Effects Models. In many respects, the random effects structure provides a more flexible framework to consider many of the traditional experimental designs as well as many non-traditional designs with the benefit of more easily assessing variability at each hierarchical level.</p>
<p>Mixed effects models combine what we call “fixed” and “random” effects.</p>
<table>
<colgroup>
<col width="27%" />
<col width="72%" />
</colgroup>
<tbody>
<tr class="odd">
<td><p><strong>Fixed effects</strong></p></td>
<td><p>Unknown constants that we wish to estimate from the model and could be similarly estimated in subsequent experimentation. The research is interested in these particular levels.</p></td>
</tr>
<tr class="even">
<td><p><strong>Random effects</strong></p></td>
<td><p>Random variables sampled from a population which cannot be observed in subsequent experimentation. The research is not interested in these particular levels, but rather how the levels vary from sample to sample.</p></td>
</tr>
</tbody>
</table>
<p>For example, in a rabbit study that examined the effect of diet on the growth of domestic rabbits and we had 10 litters of rabbits and used the 3 most similar from each litter to test 6 different diets. Here, the 6 different diets are fixed effects because they are not randomly selected from a population, these exact same diets can be further studied, and these are the diets we are interested it. The litters of rabbits and the individual rabbits are randomly selected from populations, cannot be exactly replicated in future studies, and we are not interested in the individual litters but rather what the variability is between individuals and between litters.</p>
<p>Often random effects are not of primary interest to the researcher, but must be considered. Often blocking variables are random effects because the arise from a random sample of possible blocks that are potentially available to the researcher.</p>
<p>Mixed effects models are models that have both fixed and random effects. We will first concentrate on understanding how to address a model with two sources error and then complicate the matter with fixed effects.</p>
<div id="review-of-maximum-likelihood-methods" class="section level2">
<h2><span class="header-section-number">11.1</span> Review of Maximum Likelihood Methods</h2>
<p>Recall that the likelihood function is the function links the model parameters to the data and is found by taking the probability density function and interpreting it as a function of the parameters instead of the a function of the data. Loosely, the probability function tells us what outcomes are most probable, with the height of the function telling us which values (or regions of values) are most probable given a set of parameter values. The higher the probability function, the higher the probability of seeing that value (or data in that region). The likelihood function turns that relationship around and tells us what parameter values are most likely to have generated the data we have, again with the parameter values with a higher likelihood value being more “likely”.</p>
<p>The likelihood function for a sample <span class="math inline">\(y_i \stackrel{iid}{\sim} N\left( 0, \sigma \right)\)</span> is If we regard this as a function of our parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^{2}\)</span> then we have defined our likelihood function <span class="math display">\[L \left(\mu,\sigma^{2}|y_{1},\dots,y_{n}\right)=\frac{1}{\left(2\pi\right)^{n/2}\left[\det\left(\boldsymbol{\Omega}\right)\right]^{1/2}}\exp\left[-\frac{1}{2}\left(\boldsymbol{y}-\boldsymbol{\mu}\right)^{T}\boldsymbol{\Omega}^{-1}\left(\boldsymbol{y}-\boldsymbol{\mu}\right)\right]\]</span></p>
<p>where the variance/covariance matrix is <span class="math inline">\(\boldsymbol{\Omega}=\sigma I_n\)</span>.</p>
<p>We can use to this equation to find the maximum likelihood estimators by either taking the derivatives and setting them equal to zero and solving for the parameters or by using numerical methods. In the normal case, we can find the maximum likelihood estimators (MLEs) using the derivative trick and we find that <span class="math display">\[\hat{\mu}_{MLE}=\hat{y}=\bar{y}\]</span> and <span class="math display">\[\hat{\sigma}_{MLE}^{2}=\frac{1}{n}\sum_{i=1}^{n}\left(y_{i}-\hat{y}\right)^{2}\]</span> and we notice that this is not our usual estimator <span class="math inline">\(\hat{\sigma}^{2}=s^{2}\)</span> where <span class="math inline">\(s^{2}\)</span> is the sample variance. It turns out that the MLE estimate of <span class="math inline">\(\sigma^{2}\)</span> is biased (the correction is to divide by <span class="math inline">\(n-1\)</span> instead of <span class="math inline">\(n\)</span>). This is normally not an issue if our sample size is large, but with a small sample, the bias is not insignificant.</p>
<p>Notice if we happened to know that <span class="math inline">\(\mu=0\)</span>, then we could use <span class="math display">\[\hat{\sigma}_{MLE}^{2}=\frac{1}{n}\sum_{i=1}^{n}y_{i}^{2}\]</span> and this would be unbiased for <span class="math inline">\(\sigma^{2}\)</span>.</p>
<p>In general (a not just in the normal case above) the <em>Likelihood Ratio Test</em> (LRT) provides a way for us to compare two nested models. Given <span class="math inline">\(m_{0}\)</span> which is a simplification of <span class="math inline">\(m_{1}\)</span> then we could calculate the likelihoods functions of the two models <span class="math inline">\(L\left(\boldsymbol{\theta}_{0}\right)\)</span> and <span class="math inline">\(L\left(\boldsymbol{\theta}_{1}\right)\)</span> where <span class="math inline">\(\boldsymbol{\theta}_{0}\)</span> is a vector of parameters for the null model and <span class="math inline">\(\boldsymbol{\theta}_{1}\)</span> is a vector of parameter for the alternative. Let <span class="math inline">\(\hat{\boldsymbol{\theta}}_{0}\)</span> be the maximum likelihood estimators for the null model and <span class="math inline">\(\hat{\boldsymbol{\theta}}_{1}\)</span> be the maximum likelihood estimators for the alternative. Finally we consider the value of <span class="math display">\[\begin{aligned}
  D &amp;=  -2*\log\left[\frac{L\left(\hat{\boldsymbol{\theta}}_{0}\right)}{L\left(\hat{\boldsymbol{\theta}}_{1}\right)}\right] \\
      &amp;=    -2\left[\log L\left(\hat{\boldsymbol{\theta}}_{0}\right)-\log L\left(\hat{\boldsymbol{\theta}}_{1}\right)\right]
    \end{aligned}\]</span></p>
<p>Under the null hypothesis that <span class="math inline">\(m_{0}\)</span> is the true model, the <span class="math inline">\(D\stackrel{\cdot}{\sim}\chi_{p_{1}-p_{0}}^{2}\)</span> where <span class="math inline">\(p_{1}-p_{0}\)</span> is the difference in number of parameters in the null and alternative models. That is to say that asymptotically <span class="math inline">\(D\)</span> has a Chi-squared distribution with degrees of freedom equal to the difference in degrees of freedom of the two models.</p>
<p>We could think of <span class="math inline">\(L\left(\hat{\boldsymbol{\theta}}_{0}\right)\)</span> as the maximization of the likelihood when some parameters are held constant (at zero) and all the other parameters are vary. But we are not required to hold it constant at zero. We could chose any value of interest and perform a LRT.</p>
<p>Because we often regard a confidence interval as the set of values that would not be rejected by a hypothesis test, we could consider a sequence of possible values for a parameter and figure out which would not be rejected by the LRT. In this fashion we can construct confidence intervals for parameter values.</p>
<p>Unfortunately all of this hinges on the asymptotic distribution of <span class="math inline">\(D\)</span> and often this turns out to be a poor approximation. In simple cases more exact tests can be derived (for example the F-tests we have used prior) but sometimes nothing better is currently known. Another alternative is to use permutation methods.</p>
</div>
<div id="way-anova-with-a-random-effect" class="section level2">
<h2><span class="header-section-number">11.2</span> 1-way ANOVA with a random effect</h2>
<p>We first consider the simplest model with two sources of variability, a 1-way ANOVA with a random factor covariate <span class="math display">\[y_{ij}=\mu+\gamma_{i}+\epsilon_{ij}\]</span> where <span class="math inline">\(\gamma_{i}\stackrel{iid}{\sim}N\left(0,\sigma_{\gamma}^{2}\right)\)</span> and <span class="math inline">\(\epsilon_{ij}\stackrel{iid}{\sim}N\left(0,\sigma_{\epsilon}^{2}\right)\)</span>. This model could occur, for example, when looking at the adult weight of domestic rabbits where the random effect is the effect of litter and we are interested in understanding how much variability there is between litters <span class="math inline">\(\left(\sigma_{\gamma}^{2}\right)\)</span> and how much variability there is within a litter <span class="math inline">\(\left(\sigma_{\epsilon}^{2}\right)\)</span>. Another example is the the creation of computer chips. Here a single waffer of silicon is used to create several chips and we might have waffer-to-waffer variability and then within a waffer, you have chip-to-chip variability.</p>
<p>First we should think about what the variances and covariances are for any two observations. <span class="math display">\[\begin{aligned}
  Var\left(y_{ij}\right)    
   &amp;=   Var\left(\mu+\gamma_{i}+\epsilon_{ij}\right) \\
     &amp;= Var\left(\mu\right)+Var\left(\gamma_{i}\right)+Var\left(\epsilon_{ij}\right) \\
     &amp;= 0+\sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} 
     \end{aligned}\]</span> and <span class="math inline">\(Cov\left(y_{ij},y_{ik}\right)=\sigma_{\gamma}^{2}\)</span> because the two observations share the same litter <span class="math inline">\(\gamma_{i}\)</span>. For two observations in different litters, the covariance is 0. These relationships induce a correlation on observations within the same litter of <span class="math display">\[\rho=\frac{\sigma_{\gamma}^{2}}{\sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2}}\]</span></p>
<p>For example, suppose that we have <span class="math inline">\(I=3\)</span> litters and in each litter we have <span class="math inline">\(J=3\)</span> rabbits per litter. Then the variance-covariance matrix looks like <span class="math display">\[\boldsymbol{\Omega}   =   \left[\begin{array}{ccccccccc}
\sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} &amp; \sigma_{\gamma}^{2} &amp; \sigma_{\gamma}^{2} &amp; . &amp; . &amp; . &amp; . &amp; . &amp; .\\
\sigma_{\gamma}^{2} &amp; \sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} &amp; \sigma_{\gamma}^{2} &amp; . &amp; . &amp; . &amp; . &amp; . &amp; .\\
\sigma_{\gamma}^{2} &amp; \sigma_{\gamma}^{2} &amp; \sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} &amp; . &amp; . &amp; . &amp; . &amp; . &amp; .\\
. &amp; . &amp; . &amp; \sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} &amp; \sigma_{\gamma}^{2} &amp; \sigma_{\gamma}^{2} &amp; . &amp; . &amp; .\\
. &amp; . &amp; . &amp; \sigma_{\gamma}^{2} &amp; \sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} &amp; \sigma_{\gamma}^{2} &amp; . &amp; . &amp; .\\
. &amp; . &amp; . &amp; \sigma_{\gamma}^{2} &amp; \sigma_{\gamma}^{2} &amp; \sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} &amp; . &amp; . &amp; .\\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; \sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} &amp; \sigma_{\gamma}^{2} &amp; \sigma_{\gamma}^{2}\\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; \sigma_{\gamma}^{2} &amp; \sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} &amp; \sigma_{\gamma}^{2}\\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; \sigma_{\gamma}^{2} &amp; \sigma_{\gamma}^{2} &amp; \sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2}
\end{array}\right]\]</span></p>
<p>Substituting this new variance-covariance matrix into our likelihood function, we now have a likelihood function which we can perform our usual MLE tricks with.</p>
<p>In the more complicated situation where we have a full mixed effects model, we could write <span class="math display">\[\boldsymbol{y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{Z}\boldsymbol{\gamma}+\boldsymbol{\epsilon}\]</span> where <span class="math inline">\(\boldsymbol{X}\)</span> is the design matrix for the fixed effects, <span class="math inline">\(\boldsymbol{\beta}\)</span> is the vector of fixed effect coefficients, <span class="math inline">\(\boldsymbol{Z}\)</span> is the design matrix for random effects, <span class="math inline">\(\boldsymbol{\gamma}\)</span> is the vector of random effects such that <span class="math inline">\(\gamma_{i}\stackrel{iid}{\sim}N\left(0,\sigma_{\gamma}^{2}\right)\)</span> and finally <span class="math inline">\(\boldsymbol{\epsilon}\)</span> is the vector of error terms such that <span class="math inline">\(\epsilon_{ij}\stackrel{iid}{\sim}N\left(0,\sigma_{\epsilon}^{2}\right)\)</span>. Notice in our rabbit case</p>
<p><span class="math display">\[\boldsymbol{Z}=\left[\begin{array}{ccc}
1 &amp; \cdot &amp; \cdot\\
1 &amp; \cdot &amp; \cdot\\
1 &amp; \cdot &amp; \cdot\\
\cdot &amp; 1 &amp; \cdot\\
\cdot &amp; 1 &amp; \cdot\\
\cdot &amp; 1 &amp; \cdot\\
\cdot &amp; \cdot &amp; 1\\
\cdot &amp; \cdot &amp; 1\\
\cdot &amp; \cdot &amp; 1
\end{array}\right]\;\;\;\;ZZ^{T}=\left[\begin{array}{ccccccccc}
1 &amp; 1 &amp; 1 &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot\\
1 &amp; 1 &amp; 1 &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot\\
1 &amp; 1 &amp; 1 &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot\\
\cdot &amp; \cdot &amp; \cdot &amp; 1 &amp; 1 &amp; 1 &amp; \cdot &amp; \cdot &amp; \cdot\\
\cdot &amp; \cdot &amp; \cdot &amp; 1 &amp; 1 &amp; 1 &amp; \cdot &amp; \cdot &amp; \cdot\\
\cdot &amp; \cdot &amp; \cdot &amp; 1 &amp; 1 &amp; 1 &amp; \cdot &amp; \cdot &amp; \cdot\\
\cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; 1 &amp; 1 &amp; 1\\
\cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; 1 &amp; 1 &amp; 1\\
\cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; 1 &amp; 1 &amp; 1
\end{array}\right]\]</span></p>
<p>which makes it easy to notice <span class="math display">\[\boldsymbol{\Omega}=\sigma_{\gamma}^{2}\boldsymbol{Z}\boldsymbol{Z}^{T}+\sigma_{\epsilon}^{2}\boldsymbol{I}\]</span></p>
<p>In practice we tend to have relatively small numbers of block parameters and thus have a small number of observations in which to estimate <span class="math inline">\(\sigma_{\gamma}^{2}\)</span> which means that the biased nature of MLE estimates will be suboptimal. If we knew that <span class="math inline">\(\boldsymbol{X}\boldsymbol{\beta}=\boldsymbol{0}\)</span> we could use that fact and have an unbiased estimate of our variance parameters. Because <span class="math inline">\(\boldsymbol{X}\)</span> is known, we can find linear functions <span class="math inline">\(\boldsymbol{k}\)</span> such that <span class="math inline">\(\boldsymbol{k}^{T}\boldsymbol{X}=0\)</span>. We can form a matrix <span class="math inline">\(\boldsymbol{K}\)</span> that represents all of these possible transformations and we notice that <span class="math display">\[\boldsymbol{K}^{T}\boldsymbol{y} \sim N \left( \boldsymbol{K}^{T}\boldsymbol{X\beta}, \, \boldsymbol{K}^{T}\boldsymbol{\Omega}\boldsymbol{K}\right) = N\left( \boldsymbol{0}, \boldsymbol{K}^{T}\boldsymbol{\Omega}\boldsymbol{K}\right)\]</span> and perform our maximization on this transformed set of data. Once we have our unbiased estimates of <span class="math inline">\(\sigma_{\gamma}^{2}\)</span> and <span class="math inline">\(\sigma_{\epsilon}^{2}\)</span>, we can substitute these back into the untransformed likelihood function and find the MLEs for <span class="math inline">\(\boldsymbol{\beta}\)</span>. This process is called Restricted Maximum Likelihood (REML) and is generally preferred over the variance component estimates found simply maximizing the regular likelihood function. As usual, if our experiment is balanced these complications aren’t necessary as the REML estimates of <span class="math inline">\(\boldsymbol{\beta}\)</span> are usually the same as the ML estimates.</p>
<p>Our first example comes from an experiment to test the paper brightness as affected by the shift operator. The data has 20 observations with 4 different operators. Each operator had 5 different observations made. The data set is <code>pulp</code> in the package <code>faraway</code>. We will first analyze this using a fixed-effects one-way ANOVA, but we will use a different model representation. Instead of using the first operator as the reference level, we will use the sum-to-zero constraint (to make it easier to compare with the output of the random effects model).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(faraway)
<span class="kw">data</span>(pulp)
<span class="co"># set the contrasts to sum-to-zero constraint</span>
op &lt;-<span class="st"> </span><span class="kw">options</span>(<span class="dt">contrasts=</span><span class="kw">c</span>(<span class="st">&#39;contr.sum&#39;</span>, <span class="st">&#39;contr.poly&#39;</span>))
m &lt;-<span class="st"> </span><span class="kw">aov</span>(bright ~<span class="st"> </span>operator, <span class="dt">data=</span>pulp)
<span class="kw">summary</span>(m)</code></pre></div>
<pre><code>##             Df Sum Sq Mean Sq F value Pr(&gt;F)  
## operator     3   1.34  0.4467   4.204 0.0226 *
## Residuals   16   1.70  0.1062                 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(m)</code></pre></div>
<pre><code>## (Intercept)   operator1   operator2   operator3 
##       60.40       -0.16       -0.34        0.22</code></pre>
<p>The sum-to-zero constraint forces the operator parameters to sum to zero so we can find the value of the fourth operator as operator4 = -(-0.16-0.34+0.22) = 0.28</p>
<p>To fit the random effects model we will use the package <code>lme4</code> which stands for Linear Mixed Effects and the 4 represents that the package is written in the S4 dialect of R (which means a few things will be new to us).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m2 &lt;-<span class="st"> </span><span class="kw">lmer</span>( bright ~<span class="st"> </span><span class="dv">1</span> +<span class="st"> </span>(<span class="dv">1</span>|operator), <span class="dt">data=</span>pulp )
<span class="kw">summary</span>(m2)</code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: bright ~ 1 + (1 | operator)
##    Data: pulp
## 
## REML criterion at convergence: 18.6
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -1.4666 -0.7595 -0.1244  0.6281  1.6012 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  operator (Intercept) 0.06808  0.2609  
##  Residual             0.10625  0.3260  
## Number of obs: 20, groups:  operator, 4
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  60.4000     0.1494   404.2</code></pre>
<p>Notice that the estimate of the fixed effect (the overall mean) is the same in the fixed-effects ANOVA and in the mixed model. However the fixed effects ANOVA estimates the effect of each operator while the mixed model is interested in estimating the variance between operators. In the model statement the (1|operator) denotes the random effect and this notation tells us to fit a model with a random intercept term for each operator. Here the variance associated with the operators is <span class="math inline">\(\sigma_{\gamma}^{2}=0.068\)</span> while the “pure error” is <span class="math inline">\(\sigma_{\epsilon}^{2}=0.106\)</span>. The column for standard deviation is not the variability associated with our estimate, but is simply the square-root of the variance terms <span class="math inline">\(\sigma_{\gamma}\)</span> and <span class="math inline">\(\sigma_{\epsilon}\)</span>. This was fit using the REML method.</p>
<p>We might be interested in the estimated effect of each operator</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ranef</span>(m2)</code></pre></div>
<pre><code>## $operator
##   (Intercept)
## a  -0.1219403
## b  -0.2591231
## c   0.1676679
## d   0.2133955</code></pre>
<p>These effects are smaller than the values we estimated in the fixed effects model due to distributional assumption that penalizes large deviations from the mean. In general, the estimated random effects are of smaller magnitude than the effect size estimated using a fixed effect model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># reset the contrasts to the default</span>
<span class="kw">options</span>(<span class="dt">contrasts=</span><span class="kw">c</span>(<span class="st">&quot;contr.treatment&quot;</span>, <span class="st">&quot;contr.poly&quot;</span> ))</code></pre></div>
</div>
<div id="blocks-as-random-variables" class="section level2">
<h2><span class="header-section-number">11.3</span> Blocks as Random Variables</h2>
<p>Blocks are properties of experimental designs and usually we are not interested in the block levels per se but need to account for the variability introduced by them.</p>
<p>Recall the agriculture experiment in the dataset <code>oatvar</code> from the <code>faraway</code> package. We had 8 different varieties of oats and we had 5 different fields (which we called blocks). Because of limitations on how we plant, we could only divide the blocks into 8 plots and in each plot we planted one of the varieties.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(faraway)
<span class="kw">ggplot</span>(oatvar, <span class="kw">aes</span>(<span class="dt">y=</span>yield, <span class="dt">x=</span> variety)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">facet_wrap</span>(~block, <span class="dt">labeller=</span>label_both)</code></pre></div>
<p><img src="Statistical_Methods_II_files/figure-html/unnamed-chunk-182-1.png" width="672" /></p>
<p>In this case, we don’t really care about these particular fields (blocks) and would prefer to think about these as a random sample of fields that we might have used in our experiment.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model<span class="fl">.0</span> &lt;-<span class="st"> </span><span class="kw">lmer</span>( yield ~<span class="st"> </span>(<span class="dv">1</span>|block), <span class="dt">data=</span>oatvar)
model<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">lmer</span>( yield ~<span class="st"> </span>variety +<span class="st"> </span>(<span class="dv">1</span>|block), <span class="dt">data=</span>oatvar) 
<span class="kw">summary</span>(model<span class="fl">.1</span>)</code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: yield ~ variety + (1 | block)
##    Data: oatvar
## 
## REML criterion at convergence: 341.4
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -1.7135 -0.5503 -0.1280  0.4862  2.1756 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  block    (Intercept)  876.5   29.61   
##  Residual             1336.9   36.56   
## Number of obs: 40, groups:  block, 5
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)   334.40      21.04  15.894
## variety2       42.20      23.12   1.825
## variety3       28.20      23.12   1.219
## variety4      -47.60      23.12  -2.058
## variety5      105.00      23.12   4.541
## variety6       -3.80      23.12  -0.164
## variety7      -16.00      23.12  -0.692
## variety8       49.80      23.12   2.154
## 
## Correlation of Fixed Effects:
##          (Intr) varty2 varty3 varty4 varty5 varty6 varty7
## variety2 -0.550                                          
## variety3 -0.550  0.500                                   
## variety4 -0.550  0.500  0.500                            
## variety5 -0.550  0.500  0.500  0.500                     
## variety6 -0.550  0.500  0.500  0.500  0.500              
## variety7 -0.550  0.500  0.500  0.500  0.500  0.500       
## variety8 -0.550  0.500  0.500  0.500  0.500  0.500  0.500</code></pre>
<p>We start with the Random effects. This section shows us the <em>block-to-block</em> variability (and the square root of that, the Standard Deviation) as well as the “pure-error”, labeled residuals, which is an estimate of the variability associated with two different observations (after the difference in variety is accounted for) planted <em>within</em> the same block. For this we see that block-to-block variability is only slightly smaller than the within block variability.</p>
<p>Why do we care about this? This actually tells us quite a lot about the spatial variability. Because yield is affected by soil nutrients, micro-climate, soil water availability, etc, I expect that two identical seedlings planted in slightly different conditions will have slightly different yields. By examining how the yield changes over small distances (the residual within block variability) vs how it changes over long distances (block to block variability) we can get a sense as to the scale at which these background lurking processes operate.</p>
<p>Next we turn to the fixed effects. These will be the offsets from the reference group, as we’ve typically worked with. Here we see that varieties 2,5, and 8 are the best performers (relative to variety 1), but we don’t have any p-values denoting if these differences could be due to random chance or if the differences are statistical significant. The reason for this is that in mixed models it is not always clear what the appropriate degrees of freedom are for the residuals, and therefore we don’t know what the appropriate t-distribution is to compare the t-values to. In simple balanced designs the degrees of freedom can be calculated, but in complicated unbalanced designs the appropriate degrees of freedom is not known and all proposed heuristic methods (including what is calculated by SAS) can fail spectacularly in certain cases. The authors of <code>lme4</code> are adamant that until robust methods are developed, they prefer to not calculate any p-values. A previous version of this software (package <code>nlme</code>) will produce p-values but at the cost of flexibility in the types of mixed models that can be fit.</p>
<p>We can force R to give us a the Likelihood Ratio Test (LRT) to see if <code>variety</code> is statistically significant by comparing the appropriate models. Notice that R has noticed that we fit both models using Restricted Maximum Likelihood and it is inappropriate to compare models with different fixed effects (because the <span class="math inline">\(\boldsymbol{K}\)</span> matices are different) and it has refit the models appropriately.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(model<span class="fl">.0</span>, model<span class="fl">.1</span>)</code></pre></div>
<pre><code>## refitting model(s) with ML (instead of REML)</code></pre>
<pre><code>## Data: oatvar
## Models:
## model.0: yield ~ (1 | block)
## model.1: yield ~ variety + (1 | block)
##         Df    AIC    BIC  logLik deviance Chisq Chi Df Pr(&gt;Chisq)    
## model.0  3 446.94 452.01 -220.47   440.94                            
## model.1 10 421.67 438.56 -200.84   401.67 39.27      7  1.736e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Using the LRT test (and difference of AIC values), we see the effect of variety is statistically signficant.</p>
<p>Now that we are certain that there are differences among the varieties, we should look at all of the pairwise contrasts among the variety levels. Unfortunately <code>TukeyHSD</code> doesn’t work with objects created by <code>lme</code> so we have to turn to another package. In this case, we could use either <code>multcomp</code> and build the contrast vectors <span class="math inline">\(\boldsymbol{c}\)</span> that we used previously or we could use the package <code>lsmeans</code>, which automates much of this.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">lsmeans</span>( model<span class="fl">.1</span>, pairwise ~<span class="st"> </span>variety)</code></pre></div>
<pre><code>## Loading required namespace: pbkrtest</code></pre>
<pre><code>## $lsmeans
##  variety lsmean       SE    df lower.CL upper.CL
##  1        334.4 21.03996 15.25 289.6196 379.1804
##  2        376.6 21.03996 15.25 331.8196 421.3804
##  3        362.6 21.03996 15.25 317.8196 407.3804
##  4        286.8 21.03996 15.25 242.0196 331.5804
##  5        439.4 21.03996 15.25 394.6196 484.1804
##  6        330.6 21.03996 15.25 285.8196 375.3804
##  7        318.4 21.03996 15.25 273.6196 363.1804
##  8        384.2 21.03996 15.25 339.4196 428.9804
## 
## Confidence level used: 0.95 
## 
## $contrasts
##  contrast estimate       SE df t.ratio p.value
##  1 - 2       -42.2 23.12491 28  -1.825  0.6095
##  1 - 3       -28.2 23.12491 28  -1.219  0.9192
##  1 - 4        47.6 23.12491 28   2.058  0.4638
##  1 - 5      -105.0 23.12491 28  -4.541  0.0022
##  1 - 6         3.8 23.12491 28   0.164  1.0000
##  1 - 7        16.0 23.12491 28   0.692  0.9966
##  1 - 8       -49.8 23.12491 28  -2.154  0.4078
##  2 - 3        14.0 23.12491 28   0.605  0.9985
##  2 - 4        89.8 23.12491 28   3.883  0.0116
##  2 - 5       -62.8 23.12491 28  -2.716  0.1595
##  2 - 6        46.0 23.12491 28   1.989  0.5062
##  2 - 7        58.2 23.12491 28   2.517  0.2295
##  2 - 8        -7.6 23.12491 28  -0.329  1.0000
##  3 - 4        75.8 23.12491 28   3.278  0.0491
##  3 - 5       -76.8 23.12491 28  -3.321  0.0446
##  3 - 6        32.0 23.12491 28   1.384  0.8569
##  3 - 7        44.2 23.12491 28   1.911  0.5549
##  3 - 8       -21.6 23.12491 28  -0.934  0.9798
##  4 - 5      -152.6 23.12491 28  -6.599  &lt;.0001
##  4 - 6       -43.8 23.12491 28  -1.894  0.5658
##  4 - 7       -31.6 23.12491 28  -1.366  0.8644
##  4 - 8       -97.4 23.12491 28  -4.212  0.0051
##  5 - 6       108.8 23.12491 28   4.705  0.0014
##  5 - 7       121.0 23.12491 28   5.232  0.0003
##  5 - 8        55.2 23.12491 28   2.387  0.2859
##  6 - 7        12.2 23.12491 28   0.528  0.9994
##  6 - 8       -53.6 23.12491 28  -2.318  0.3194
##  7 - 8       -65.8 23.12491 28  -2.845  0.1237
## 
## P value adjustment: tukey method for comparing a family of 8 estimates</code></pre>
<p>This looks remarkably similar to the output from TukeyHSD and just as confusing. Ideally the function <code>make_TukeyHSD_letters()</code> could be modified to handle this model as well.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(stringr)</code></pre></div>
<pre><code>## Warning: package &#39;stringr&#39; was built under R version 3.2.5</code></pre>
<pre><code>## 
## Attaching package: &#39;stringr&#39;</code></pre>
<pre><code>## The following object is masked _by_ &#39;.GlobalEnv&#39;:
## 
##     fruit</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#&#39; Create a data frame with significant groupings </span>
<span class="co">#&#39;</span>
<span class="co">#&#39; This function runs TukeyHSD on the input model and then creates a data frame</span>
<span class="co">#&#39; with a column for the factor and a second for the Significance Group</span>
<span class="co">#&#39; </span>
<span class="co">#&#39; @param model The output of a lm(), aov(), or lme().</span>
<span class="co">#&#39; @param variable The variable of interest.</span>
<span class="co">#&#39; @output A data frame with a column for factor and another for the signicance group.</span>
<span class="co">#&#39; @examples</span>
<span class="co">#&#39; model &lt;- lm( breaks ~ wool + tension, data=warpbreaks )</span>
<span class="co">#&#39; make_TukeyHSD_letters(model, ~ wool)</span>
<span class="co">#&#39; make_TukeyHSD_letters(model, ~ tension)</span>
<span class="co">#&#39; make_TukeyHSD_letters(model, ~ wool+tension)</span>
make_TukeyHSD_letters &lt;-<span class="st"> </span>function(model, formula){ 
  formula2 &lt;-<span class="st"> </span><span class="kw">update.formula</span>( formula, pairwise ~<span class="st"> </span>.)
  temp &lt;-<span class="st"> </span><span class="kw">lsmeans</span>(model, formula2)$contrasts %&gt;%
<span class="st">    </span><span class="kw">summary</span>()  
  temp2 &lt;-<span class="st"> </span>temp$p.value
  <span class="kw">names</span>(temp2) &lt;-<span class="st"> </span>temp$contrast 
  temp &lt;-<span class="st"> </span>temp2 %&gt;%<span class="st"> </span><span class="kw">vec2mat</span>(<span class="dt">sep=</span><span class="st">&#39; - &#39;</span>) %&gt;%<span class="st"> </span><span class="kw">multcompLetters</span>()
  out &lt;-<span class="st">  </span><span class="kw">data.frame</span>(<span class="dt">group =</span> <span class="kw">names</span>(temp$Letters), <span class="dt">SigGroup=</span>temp$Letters)
  <span class="kw">colnames</span>(out)[<span class="dv">1</span>] &lt;-<span class="st">   </span><span class="kw">Reduce</span>(paste, <span class="kw">deparse</span>(formula)) %&gt;%<span class="st"> </span><span class="kw">str_replace</span>(<span class="kw">fixed</span>(<span class="st">&#39;~&#39;</span>),<span class="st">&#39;&#39;</span>)
  <span class="kw">rownames</span>(out) &lt;-<span class="st"> </span><span class="ot">NULL</span>
  out
}  </code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">make_TukeyHSD_letters</span>(model<span class="fl">.1</span>, ~variety)</code></pre></div>
<pre><code>##   variety SigGroup
## 1       1       ab
## 2       2       ac
## 3       3        a
## 4       4        b
## 5       5        c
## 6       6       ab
## 7       7       ab
## 8       8       ac</code></pre>
<p>As usual we’ll join this information into the original data table and then make a nice summary graph.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">oatvar &lt;-<span class="st"> </span>oatvar %&gt;%
<span class="st">  </span><span class="kw">left_join</span>( <span class="kw">make_TukeyHSD_letters</span>(model<span class="fl">.1</span>, ~variety) ) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">LetterHeight=</span><span class="dv">500</span>)</code></pre></div>
<pre><code>## Joining, by = c(&quot;variety&quot;, &quot;SigGroup&quot;)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(oatvar, <span class="kw">aes</span>(<span class="dt">x=</span>variety, <span class="dt">y=</span>yield)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">color=</span>block)) +
<span class="st">  </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">label=</span>SigGroup, <span class="dt">y=</span>LetterHeight))</code></pre></div>
<p><img src="Statistical_Methods_II_files/figure-html/unnamed-chunk-188-1.png" width="672" /></p>
<hr />
<p>We’ll consider a second example using data from the pharmaceutical industry. We are interested in 4 different processes (our treatment variable) used in the biosynthesis and purification of the drug penicillin. The biosynthesis requires a nutrient source (corn steep liquor) as a nutrient source for the fungus and the nutrient source is quite variable. Each batch of the nutrient is is referred to as a ‘blend’ and each blend is sufficient to create 4 runs of penicillin. We avoid confounding our biosynthesis methods with the blend by using a Randomized Complete Block Design and observing the yield of penicillin from each of the four methods (A,B,D, and D) in each blend.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(penicillin)
<span class="kw">ggplot</span>(penicillin, <span class="kw">aes</span>(<span class="dt">y=</span>yield, <span class="dt">x=</span>treat)) +
<span class="st">  </span><span class="kw">geom_point</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">facet_wrap</span>( ~<span class="st"> </span>blend, <span class="dt">ncol=</span><span class="dv">5</span>)</code></pre></div>
<p><img src="Statistical_Methods_II_files/figure-html/unnamed-chunk-189-1.png" width="672" /></p>
<p>It looks like there is definately a <code>Blend</code> effect (e.g. Blend1 is much better than Blend5) but it isn’t clear that there is a treatment effect.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model<span class="fl">.0</span> &lt;-<span class="st"> </span><span class="kw">lmer</span>(yield ~<span class="st">  </span><span class="dv">1</span>    +<span class="st"> </span>(<span class="dv">1</span> |<span class="st"> </span>blend), <span class="dt">data=</span>penicillin)
model<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">lmer</span>(yield ~<span class="st"> </span>treat +<span class="st"> </span>(<span class="dv">1</span> |<span class="st"> </span>blend), <span class="dt">data=</span>penicillin)
<span class="kw">anova</span>(model<span class="fl">.0</span>, model<span class="fl">.1</span>)</code></pre></div>
<pre><code>## refitting model(s) with ML (instead of REML)</code></pre>
<pre><code>## Data: penicillin
## Models:
## model.0: yield ~ 1 + (1 | blend)
## model.1: yield ~ treat + (1 | blend)
##         Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
## model.0  3 127.33 130.31 -60.662   121.33                         
## model.1  6 129.28 135.25 -58.639   117.28 4.0474      3     0.2564</code></pre>
<p>It looks like we don’t have a significant effect of the treatments. Next we’ll examine the simple model to understand the variability.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(model<span class="fl">.0</span>)</code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: yield ~ 1 + (1 | blend)
##    Data: penicillin
## 
## REML criterion at convergence: 118.4
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -1.5526 -0.7310 -0.0789  0.5007  1.8241 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  blend    (Intercept) 11.57    3.401   
##  Residual             19.73    4.442   
## Number of obs: 20, groups:  blend, 5
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)   86.000      1.817   47.34</code></pre>
<p>We see that the noise is more in the within blend rather than the between blends. If my job were to understand the variability and figure out how to improve production, this suggests that understanding the both how variability is introduced at the blend level <em>and</em> at the run level. The run level has slightly more variability, so I might start there.</p>
</div>
<div id="nested-effects" class="section level2">
<h2><span class="header-section-number">11.4</span> Nested Effects</h2>
<p>When the levels of one factor vary only within the levels of another factor, that factor is said to be nested. For example, when measuring the performance of workers at several job locations, if the workers only work at one site, then the workers are nested within site. If the workers work at more than one location, we would say that workers are crossed with site.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="10-block-designs.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>


<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dereksonderegger/STA_571_Book/raw/master/11_RandomEffects.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
